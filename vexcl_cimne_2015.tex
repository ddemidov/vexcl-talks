\documentclass[@BEAMER_OPTIONS@]{beamer}
    @USE_PGFPAGES@

    \usetheme[alternativetitlepage=true,titleline=true]{Torino}
    \setbeamertemplate{navigation symbols}{}
    \setbeamertemplate{note page}[plain]
    \setbeamertemplate{caption}{\insertcaption}

    \usepackage[utf8]{inputenc}
    \usepackage{graphicx}
    \usepackage{subfigure}
    \usepackage{xspace}
    \usepackage{adjustbox}
    \usepackage{tikz}
    \usepackage{relsize}
    \usepackage{fancyvrb}
    \fvset{fontsize=\footnotesize}
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{}
    \usepgflibrary{arrows}
    \usetikzlibrary{shadows,decorations.pathreplacing,patterns,shapes}
    \tikzstyle{every picture}=[semithick,>=stealth,remember picture]
    \usepackage{inconsolata}
    \usepackage{listings}
    \lstset{
        language=C++,
        basicstyle=\footnotesize\ttfamily,
        keywordstyle=\color{chameleon1}\bfseries,
        commentstyle=\color{chameleon3}\it\rmfamily,
        stringstyle=\color{chameleon3},
        numbers=left,
        numberstyle=\tiny,
        aboveskip=-0.02\baselineskip,
        belowskip=-0.02\baselineskip,
        columns=flexible,
        extendedchars=false,
        showstringspaces=false,
        morekeywords={global,kernel,ulong,size_t,get_global_id,get_global_size}
        }
    \newcommand{\code}[1]{\lstinline|#1|}
    \protected\def\plusplus{{\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\relsize{-3}\bf ++}}\xspace}
    \newcommand{\CXX}{{\rm C}\plusplus}
    \newcommand{\CC}{{\rm C99}\xspace}

    \input{ribbon}
    \newcommand{\forkme}{\ribbon{east}{chameleon1}{\href{https://github.com/ddemidov/vexcl}{Fork me on GitHub}}}
    \newcommand{\singledevice}{\ribbon{east}{chameleon3}{Single device only}}
    \newcommand{\additive}{\ribbon{east}{chameleon3}{Additive expressions}}

    \tikzset{
        treenode/.style={
            draw,
            fill=white,
            blur shadow,
            shadow xshift=1pt,
            shadow yshift=-1pt,
            shadow blur radius=2pt,
            shadow opacity=40
            }
        }


    \title{VexCL}
    \subtitle{GPGPU Without the Agonizing Pain}

    \author{Denis Demidov}
    \institute{
        Institute of System Research,\\Russian Academy of Sciences
        \\ \vspace{\baselineskip}
        }
    \date{
        \href{http://meetingcpp.com/index.php/schedule14.html}{CIMNE, Barcelona, 09.2015}
    }


\begin{document}

%----------------------------------------------------------------------------
\begin{frame}{}
    \titlepage
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\section{Introduction}
\begin{frame}{Modern GPGPU frameworks}
    \begin{columns}
        \begin{column}{0.45\textwidth}
            \begin{block}{CUDA}
                \begin{itemize}
                    \item Proprietary architecture by NVIDIA
                    \item Requires NVIDIA hardware
                    \item More mature, many libraries
                    \item Kernels are written in \CXX
                        \vspace{\baselineskip}
                    \item<2> \emph{Kernels are compiled to PTX together with
                        host program}
                \end{itemize}
            \end{block}
        \end{column}
        \begin{column}{0.45\textwidth}
            \begin{block}{OpenCL}
                \begin{itemize}
                    \item Open standard
                    \item Supports wide range of hardware
                    \item Code is much more verbose
                    \item Kernels are written in \CC
                        \vspace{\baselineskip}
                    \item<2> \emph{Kernels are compiled at runtime, adding an
                        initialization overhead}
                \end{itemize}
            \end{block}
        \end{column}
    \end{columns}
    \vspace{\baselineskip}
    \pause
    \begin{itemize}
        \item The latter distinction is usually considered to be an OpenCL
            drawback.
        \item But it also allows us to generate more efficient kernels at
            runtime!
            \begin{itemize}
                \item VexCL takes care of this part.
            \end{itemize}
    \end{itemize}
\end{frame}

\note[itemize]{
\item Today, major GPGPU programming frameworks are NVIDIA CUDA and OpenCL
\item ...
\item The latter distinction allows one to generate an OpenCL kernel tailored
    for the problem at hand.  And that is what I am going to talk about today.
}

%----------------------------------------------------------------------------
\begin{frame}{VexCL~--- a vector expression template library for OpenCL/CUDA}
    \forkme

    \begin{itemize}
        \item Created for ease of \CXX based GPGPU development:
            \begin{itemize}
                \item Convenient notation for vector expressions
                \item OpenCL/CUDA JIT code generation
                \item Easily combined with existing libraries/code
                \item Header-only
            \end{itemize}
            \vspace{\baselineskip}
        \item Supported backends:
            \begin{itemize}
                \item OpenCL (Khronos \CXX bindings)
                \item OpenCL (Boost.Compute)
                \item NVIDIA CUDA
            \end{itemize}
            \vspace{\baselineskip}
        \item The source code is available under MIT license:
            \begin{itemize}
                \item \href{https://github.com/ddemidov/vexcl}{https://github.com/ddemidov/vexcl}
            \end{itemize}
            \vspace{\baselineskip}
    \end{itemize}
\end{frame}

\note[itemize]{
\item VexCL is a vector expression template library for OpenCL. It allows you
    to use convenient matlab-like notation for vector operations and it
    generates the appropriate compute kernels for you automatically.
\item The library is header-only, so you don't have to build it to use it. The
    source code of the library is available on GitHub under very liberal
    MIT license.
}

\section{Motivating example}
\begin{frame}
    \sectionpage
\end{frame}

\note{}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Hello OpenCL: vector sum}
    \begin{itemize}
        \item Compute sum of two vectors in parallel:
            \begin{itemize}
                \item \code{A}, \code{B}, and \code{C} are large vectors.
                \item Compute \code{C = A + B}.
            \end{itemize}
            \vspace{\baselineskip}
        \item Overview of (any) OpenCL solution:
            \begin{enumerate}
                \item Initialize OpenCL context
                \item Allocate memory
                \item Transfer input data
                \item Run computations
                \item Get the results
            \end{enumerate}
    \end{itemize}
\end{frame}

\note[itemize]{
\item Let's start with a motivating example. This is classical hello world
    example for OpenCL: addition of two large vector.
\item To do anything with the OpenCL, you need to perform some standard steps,
    like context initialization, memory allocation and transfer, and the
    computations that you needed to do in the first place.
\item So let's look at how these steps are done with native OpenCL API.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Hello OpenCL: vector sum}
    \begin{exampleblock}{1. Query platforms}
        \lstinputlisting[firstnumber=10, linerange={10-14}]{code/hello-opencl.cpp}
    \end{exampleblock}
\end{frame}

\note[itemize]{
\item First, we need to enumerate OpenCL platforms. Platform is an OpenCL
    implementation by particular provider. Examples are NVIDIA, AMD or Intel
    platforms.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Hello OpenCL: vector sum}
    \begin{exampleblock}{2. Get the first available GPU}
        \lstinputlisting[firstnumber=17, linerange={17-32}]{code/hello-opencl.cpp}
    \end{exampleblock}
\end{frame}

\note[itemize]{
\item Once we have the list of platforms, we may pick a compute device that
    suites us. In this example we just get first available device that we are
    able to create an OpenCL context on.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Hello OpenCL: vector sum}
    \begin{exampleblock}{3. Create command queue}
        \lstinputlisting[firstnumber=35, linerange={35-35}]{code/hello-opencl.cpp}
    \end{exampleblock}
    \begin{exampleblock}{4. Prepare and copy input data}
        \lstinputlisting[firstnumber=38, linerange={38-48}]{code/hello-opencl.cpp}
    \end{exampleblock}
\end{frame}

\note[itemize]{
\item We also create the command queue for OpenCL. All OpenCL operations are
    submitted to a command queue. In general, operations submitted to the same
    queue are performed sequentially.
\item Next, we need to allocate some device memory and transfer input
    data to the device.
\item The input data is prepared at host in this example. A and B vectors would
    hold ones and twos. This is not very interesting example, but then hello
    world programs never are.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Hello OpenCL: vector sum}
    \begin{exampleblock}{5. Create kernel source}
        \lstinputlisting[firstnumber=51, linerange={51-62}]{code/hello-opencl.cpp}
    \end{exampleblock}
\end{frame}

\note[itemize]{
\item Next thing to do is to compile the compute kernels that we will use.
\item With OpenCL, the kernels are compiled at runtime, because OpenCL supports
    wide range of hardware, and each device may have its own set of
    instructions.
\item Here we create string representation of a kernel source.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Hello OpenCL: vector sum}
    \begin{exampleblock}{6. Compile the kernel}
        \lstinputlisting[firstnumber=65, linerange={65-77}]{code/hello-opencl.cpp}
    \end{exampleblock}
\end{frame}

\note[itemize]{
\item Then we compile an OpenCL program and create a kernel object.
\item We also create the command queue for OpenCL. All OpenCL operations are
    submitted to a command queue. In general, operations submitted to the same
    queue are performed sequentially.
\item And now we are basically done with initialization. All of the above
    operations are usually done once per program lifetime.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Hello OpenCL: vector sum}
    \begin{exampleblock}{7. Set kernel arguments}
        \lstinputlisting[firstnumber=80, linerange={80-83}]{code/hello-opencl.cpp}
    \end{exampleblock}
    \begin{exampleblock}{8. Launch the kernel}
        \lstinputlisting[firstnumber=86, linerange={86-86}]{code/hello-opencl.cpp}
    \end{exampleblock}
    \begin{exampleblock}{9. Get result back to host}
        \lstinputlisting[firstnumber=89, linerange={89-90}]{code/hello-opencl.cpp}
    \end{exampleblock}
\end{frame}

\note[itemize]{
\item And now we have everything we need to launch the compute kernel.
\item We set the kernel parameters and submit the kernel to the command queue.
\item Then we transfer the results back to host and see what we got. Here, if
    everything went well, we should get 'three' on standard output.
\item Well, this the ugliest hello world program I ever seen.
\item You will surely agree that a hello world program should fit on one slide.
    And 71 lines of code for a hello world is scary.
\item How much shorter do you think this example will be with VexCL?
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Hello VexCL: vector sum}
    \setbeamercovered{transparent=40}
    \vspace{-1\baselineskip}
    \begin{columns}
        \begin{column}[t]{0.2\textwidth}
            \begin{exampleblock}{OpenCL}
                \begin{adjustbox}{width=0.19\textwidth, height=\textheight, keepaspectratio}
                    \begin{minipage}{\textwidth}
                        \begin{uncoverenv}<1-2>
                            \lstinputlisting[linerange={1-8}]{code/hello-opencl.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1-2,3>
                            \lstinputlisting[firstnumber=last, linerange={9-35}]{code/hello-opencl.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1-2,4>
                            \lstinputlisting[firstnumber=last, linerange={36-48}]{code/hello-opencl.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1-2,5>
                            \lstinputlisting[firstnumber=last, linerange={49-86}]{code/hello-opencl.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1-2,6>
                            \lstinputlisting[firstnumber=last, linerange={87-90}]{code/hello-opencl.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1-2>
                            \lstinputlisting[firstnumber=last, linerange={91}]{code/hello-opencl.cpp}
                        \end{uncoverenv}
                    \end{minipage}
                \end{adjustbox}
            \end{exampleblock}
        \end{column}
        \begin{column}[t]{0.7\textwidth}
            \begin{onlyenv}<2->
            \begin{exampleblock}{VexCL}
                \begin{adjustbox}{width=0.83\textwidth, height=\textheight, keepaspectratio}
                    \begin{minipage}{\textwidth}
                        \begin{uncoverenv}<2>
                            \lstinputlisting[linerange={1-5}]{code/hello-vexcl.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<2,3>
                            \lstinputlisting[firstnumber=last, linerange={6-8}]{code/hello-vexcl.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<2,4>
                            \lstinputlisting[firstnumber=last, linerange={9-13}]{code/hello-vexcl.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<2,5>
                            \lstinputlisting[firstnumber=last, linerange={14-16}]{code/hello-vexcl.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<2,6>
                            \lstinputlisting[firstnumber=last, linerange={17-20}]{code/hello-vexcl.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<2>
                            \lstinputlisting[firstnumber=last, linerange={21}]{code/hello-vexcl.cpp}
                        \end{uncoverenv}
                    \end{minipage}
                \end{adjustbox}
            \end{exampleblock}
            \end{onlyenv}
        \end{column}
    \end{columns}
\end{frame}

\note[itemize]{
\item Here is the simplest example of using vexcl: addition of two vectors on a
    gpu card.
\item The first line is the context initialization. We provide a device filter
    to the context constructor and get all compute devices that satisfy the
    filter. Here we filter by type and get all available GPUs.
\item Data allocation and transfer is also simplified. \code{vex::vector}
    constructor allocates memory on device and possibly transfers initial data
    as well. The parameters here are list of command queues and either size or
    input host vector.
\item Line ten does what's needs to be done here. This simple expression leads
    to automatic kernel generation and launch. And then we copy the results
    back to host and see what we got.
}

%----------------------------------------------------------------------------
\section{VexCL interface}
\begin{frame}
    \sectionpage
\end{frame}

\note[itemize]{
\item Here is a brief overview of the library interface.
}

\subsection{Initialization}
%----------------------------------------------------------------------------
\begin{frame}[fragile]{Initialization}
    \begin{itemize}
        \item Multi-device and multi-platform computations are supported.
        \item VexCL context is initialized from combination of device filters.
        \item Device filter is a boolean functor acting on \code{const
            vex::backend::device&}.
    \end{itemize}
    \vspace{-0.5\baselineskip}
    \begin{overlayarea}{\textwidth}{0.4\textheight}
    \begin{exampleblock}{Initialize VexCL context on selected devices}
        \begin{onlyenv}<1>
        \begin{lstlisting}
vex::Context ctx( vex::Filter::Any );
        \end{lstlisting}
        \end{onlyenv}
        \begin{onlyenv}<2|handout:0>
        \begin{lstlisting}
vex::Context ctx( vex::Filter::GPU );
        \end{lstlisting}
        \end{onlyenv}
        \begin{onlyenv}<3|handout:0>
        \begin{lstlisting}
vex::Context ctx(vex::Filter::Accelerator && vex::Filter::Platform("Intel"));
        \end{lstlisting}
        \end{onlyenv}
        \begin{onlyenv}<4|handout:0>
        \begin{lstlisting}
vex::Context ctx(
    vex::Filter::DoublePrecision &&
    [](const vex::backend::device &d) {
        return d.getInfo<CL_DEVICE_GLOBAL_MEM_SIZE>() >= 16_GB;
    });
        \end{lstlisting}
        \end{onlyenv}
    \end{exampleblock}
    \end{overlayarea}
    \begin{figure}
        \uncover<1-2>{
            \includegraphics[width=0.2\textwidth]{tesla.png}\quad
        }
        \uncover<1,2,4>{
            \includegraphics[width=0.2\textwidth]{radeon.png}\quad
        }
        \uncover<1,3>{
            \includegraphics[width=0.17\textwidth]{intel.png}
        }
    \end{figure}
\end{frame}

\note[itemize]{
\item VexCL can transparently work with several compute devices that are
    present on your system.
\item We initialize the VexCL context with a device filter. The device filter
    is a simple functor that acts on device reference and returns a boolean
    value. Several standard filters are provided and you can write your own
    filters.
\item Let's assume that we have an NVIDIA GPU, an AMD GPU, and an Intel CPU
    installed.
    \begin{enumerate}
        \item The standard 'All' Filter select any device available, so we end
            with three devices in our context.
        \item If we want to select only GPUs, then we can filter the devices by
            type.
        \item It is also possible to combine the device filters with logical
            operators.  Here we select a GPU that is provided by AMD OpenCL
            platform.
        \item And here is an example of a custom filter. Here it selects any
            device that has at least 4GB of memory.
    \end{enumerate}
}

\subsection{Memory and work splitting}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Memory and work splitting}
    \setbeamercovered{transparent=40}
    \begin{exampleblock}{}
        \begin{onlyenv}<1|handout:0>
        \begin{lstlisting}
vex::Context ctx( vex::Filter::Name("Tesla") );
        \end{lstlisting}
        \end{onlyenv}
        \begin{onlyenv}<2|handout:0>
        \begin{lstlisting}
vex::Context ctx( vex::Filter::Type(CL_DEVICE_TYPE_GPU) );
        \end{lstlisting}
        \end{onlyenv}
        \begin{onlyenv}<3>
        \begin{lstlisting}
vex::Context ctx( vex::Filter::DoublePrecision );
        \end{lstlisting}
        \end{onlyenv}
        \begin{uncoverenv}<1>
        \begin{lstlisting}[firstnumber=last]

vex::vector<double> x(ctx, N);
vex::vector<double> y(ctx, N);

x = vex::element_index() * (1.0 / N);
y = sin(2 * x) + sqrt(1 - x * x);
        \end{lstlisting}
        \end{uncoverenv}
    \end{exampleblock}
    \setbeamercovered{invisible}
    \begin{figure}
        \begin{tikzpicture}
            \draw (0,2.5) rectangle +(8,0.1);
            \draw (0,2.5) grid[step=0.1] +(8,0.1);
            \draw (-0.3,2.6) node{x};

            \draw (0,2.0) rectangle +(8,0.1);
            \draw (0,2.0) grid[step=0.1] +(8,0.1);
            \draw (-0.3,2.1) node[anchor=center]{y};

            \uncover<1-3> {
            \draw (1,0.5) node{\includegraphics[width=0.2\textwidth]{tesla.png}};
            }

            \uncover<2-3> {
            \draw (4,0.5) node{\includegraphics[width=0.2\textwidth]{radeon.png}};
            }

            \uncover<3> {
            \draw (7.5,0.5) node{\includegraphics[width=0.17\textwidth]{intel.png}};
            }

            \uncover<1|handout:0> {
            \draw[->,chameleon3,style=dashed] (0,2.7) -- (0,1.8)
                .. controls +(east:0.5) and +(north west:0.5) ..
                (1.4,1.5);
            \draw[->,chameleon3,style=dashed] (8,2.7) -- (8,1.8)
                .. controls +(west:0.5) and +(north east:0.5) ..
                (1.6,1.5);
            }

            \uncover<2|handout:0> {
            \draw[->,chameleon3,style=dashed] (0,2.7) -- (0,1.8)
                .. controls +(east:0.5) and +(north west:0.5) ..
                (1.4,1.5);
            \draw[->,chameleon3,style=dashed] (4,2.7) -- (4,1.8)
                .. controls +(west:0.5) and +(north east:0.5) ..
                (1.6,1.5);

            \draw[->,chameleon3,style=dashed] (4,2.7) -- (4,1.8)
                .. controls +(east:0.1) and +(north west:0.2) ..
                (4.4,1.5);
            \draw[->,chameleon3,style=dashed] (8,2.7) -- (8,1.8)
                .. controls +(west:0.5) and +(north east:0.5) ..
                (4.6,1.5);
            }

            \uncover<3> {
            \draw[->,chameleon3,style=dashed] (0,2.7) -- (0,1.8)
                .. controls +(east:0.5) and +(north west:0.5) ..
                (1.4,1.5);
            \draw[->,chameleon3,style=dashed] (3,2.7) -- (3,1.8)
                .. controls +(west:0.5) and +(north east:0.5) ..
                (1.6,1.5);

            \draw[->,chameleon3,style=dashed] (3,2.7) -- (3,1.8)
                .. controls +(east:0.5) and +(north west:0.2) ..
                (4.4,1.5);
            \draw[->,chameleon3,style=dashed] (6,2.7) -- (6,1.8)
                .. controls +(west:0.5) and +(north east:0.5) ..
                (4.6,1.5);

            \draw[->,chameleon3,style=dashed] (6,2.7) -- (6,1.8) -- (7.4,1.5);
            \draw[->,chameleon3,style=dashed] (8,2.7) -- (8,1.8) -- (7.6,1.5);
            }
        \end{tikzpicture}
    \end{figure}
\end{frame}

\note[itemize]{
\item Now that we know how to initialize VexCL context, let's see how device
    vectors are allocated.
\item Here we allocate three vectors, and initialize two of them with
    constant values.
\item Each vector receives a list of queues at initialization.  Since each
    queue corresponds to a specific device, vectors know where to put their
    data to.
    \begin{enumerate}
        \item For example, if we only have the Tesla card in our context, then
            it will hold the complete memory for all of our vectors.
        \item If we use both of the available GPUs, then the vectors will be
            split between the devices. This split is by default proportional to
            the GPU bandwidth and is guaranteed to be consistent for vectors of
            the same size. This consistency allows VexCL to run computations
            independently on all devices in context.
        \item If we add the CPU to the context, it will get smaller share of
            the data and arithmetic operations.
    \end{enumerate}
\item Care must be taken with the use of several devices. VexCL tries to split
    the memory as fair as it can, but it is probable that your program will
    run at the speed of the slowest device.
}

\subsection{Copying memory between host and devices}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Copies between host and device memory}
    \begin{exampleblock}{}
        \begin{lstlisting}
vex::vector<double> d(ctx, n);
std::vector<double> h(n);
double a[100];
        \end{lstlisting}
    \end{exampleblock}
    \vspace{\baselineskip}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{exampleblock}{STL-like range copies}
                \begin{lstlisting}
vex::copy(d.begin(), d.end(), h.begin());
vex::copy(d.begin(), d.begin() + 100, a);
                \end{lstlisting}
            \end{exampleblock}
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{exampleblock}{Simple copies}
                \begin{lstlisting}
vex::copy(d, h);
vex::copy(h, d);
                \end{lstlisting}
            \end{exampleblock}
        \end{column}
    \end{columns}
    \vspace{\baselineskip}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{exampleblock}{Map OpenCL buffer to host pointer}
                \begin{lstlisting}
auto p = d.map(devnum);
std::sort(&p[0], &p[d.part_size(devnum)]);
                \end{lstlisting}
            \end{exampleblock}
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{exampleblock}{Access single element (\emph{slow})}
                \begin{lstlisting}
double v = d[42];
d[0] = 0;
                \end{lstlisting}
            \end{exampleblock}
        \end{column}
    \end{columns}
\end{frame}

\note[itemize]{
\item Copies between host and device memory may be done with simple copy
    function that copy the complete vector either way,
\item or, if you need to do partial copy, you can use STL-like syntax.
\item Vectors also overload array subscript operator, so you can have direct
    read or write access to any element of a vector. But this should be used
    with caution because it is slow. The intended use for this is a single
    element access or debugging.
\item Data may also be accessed through iterators, so it is possible to use,
    for example, an STL algorithm with device vector as a temporary solution.
}

\subsection{Vector expressions}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{What vector expressions are supported?}
    \begin{itemize}
        \item All vectors in an expression have to be \emph{compatible}:
            \begin{itemize}
                \item Have same size
                \item Located on same devices
            \end{itemize}
        \item What may be used:
            \begin{columns}
                \begin{column}{0.4\textwidth}
                    \begin{itemize}
                        \item Vectors, scalars, constants
                        \item Arithmetic, logical operators
                        \item Built-in functions
                        \item User-defined functions
                        \item Random number generators
                        \item Slicing and permutations
                    \end{itemize}
                \end{column}
                \begin{column}{0.42\textwidth}
                    \begin{itemize}
                        \item Reduce to a scalar (sum, min, max)
                        \item Reduce across chosen dimensions
                        \item Stencil operations
                        \item Sparse matrix~-- vector products
                        \item Fast Fourier Transform
                        \item Sort, scan, reduce by key
                    \end{itemize}
                \end{column}
            \end{columns}
    \end{itemize}
\end{frame}

\note[itemize]{
\item So, what kind of expressions can you use in VexCL?
\item First, any vectors used in an expression have to be compatible.
\item If this requirement is satisfied, then expressions may combine
    vectors and scalars with almost any binary operators. OpenCL math functions
    and user-defined functions are also available.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Builtin operations and functions}
    \begin{columns}
        \begin{column}{0.38\textwidth}
            \begin{exampleblock}{This expression:}
                \begin{lstlisting}
x = 2 * y - sin(z);
                \end{lstlisting}
            \end{exampleblock}
        \end{column}
        \begin{column}{0.55\textwidth}
            \begin{itemize}
                \item \code{export VEXCL_SHOW_KERNELS=1}\\
                    to see the generated code.
            \end{itemize}
        \end{column}
    \end{columns}
    \begin{exampleblock}{\ldots results in this kernel:}
        \begin{lstlisting}
kernel void vexcl_vector_kernel(
    ulong n,
    global double * prm_1,
    int prm_2,
    global double * prm_3,
    global double * prm_4
)
{
    for(size_t idx = get_global_id(0); idx < n; idx += get_global_size(0)) {
        prm_1[idx] = ( ( prm_2 * prm_3[idx] ) - sin( prm_4[idx] ) );
    }
}
        \end{lstlisting}
    \end{exampleblock}
    \begin{tikzpicture}[overlay,scale=0.6]
        \draw (16,8) node(sub)[draw,fill=white,ellipse,drop shadow]{$-$};

        \draw (sub) +(-2.00,-1) node(mul)[draw,fill=white,drop shadow,ellipse]{$*$};
        \draw (sub) +( 2.00,-1) node(sin)[draw,fill=white,drop shadow,ellipse]{sin};
        \draw (mul) +(-2.00,-1) node(two)[draw,fill=white,drop shadow,minimum size=0.5cm]{2};
        \draw (mul) +( 2.00,-1) node(y)  [draw,fill=white,drop shadow,minimum size=0.5cm]{y};
        \draw (sin) +( 1.75,-1) node(z)  [draw,fill=white,drop shadow,minimum size=0.5cm]{z};

        \draw (sub) -- (mul);
        \draw (sub) -- (sin);
        \draw (mul) -- (two);
        \draw (mul) -- (y);
        \draw (sin) -- (z);
    \end{tikzpicture}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Element indices}
    \begin{itemize}
        \item \code{vex::element_index(size_t offset = 0, size_t size = 0)}\\
            returns index of a vector element.
            \begin{itemize}
                \item The numbering starts with \code{offset} and is continuous
                    across devices.
                \item The optional \code{size} may be used to explicitly set
                    the expression size.
            \end{itemize}
    \end{itemize}
    \begin{exampleblock}{Linear function:}
        \begin{lstlisting}
vex::vector<double> X(ctx, N);
double x0 = 0, dx = 1e-3;
X = x0 + dx * vex::element_index();
        \end{lstlisting}
    \end{exampleblock}
    \begin{exampleblock}{Single period of sine function:}
        \begin{lstlisting}
X = sin(2 * M_PI / N * vex::element_index());
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note[itemize]{
\item \code{element_index} is a function that allows you to use element
    position inside of vector expressions.
\item The function may participate in arbitrary vector expressions.
\item For example, here\ldots
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{User-defined functions}
    \begin{exampleblock}{Defining a function:}
        \begin{lstlisting}
VEX_FUNCTION( double, sqr, (double, x)(double, y),
    return x * x + y * y;
    );
        \end{lstlisting}
    \end{exampleblock}
    \begin{exampleblock}{Using the function:}
        \begin{lstlisting}
Z = sqrt( sqr(X, Y) );
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note[itemize]{
\item It is possible to define an OpenCL function that may be used with vector
    expressions. You need to provide function body, parameter types, and return
    type.
\item Function body has to be of \code{extern const char} type, to allow its
    use as a template parameter. And it has to be defined at global scope.
\item Inside the body function parameters are always named prm1, prm2, etc.
\item Here we define 'between' function that returns true if its second
    parameter is between its first and third parameters. The UserFunction
    object is stateless, so it may be good idea to define it at global scope
    as well, next to its body.
\item Now we may use the function in expressions. Any vector expression may be
    used as a parameter for a user-defined (or builtin) function.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{User functions are translated to OpenCL functions}
    \begin{exampleblock}{}
        \begin{lstlisting}
Z = sqrt( sqr(X, Y) );
        \end{lstlisting}
    \end{exampleblock}
    \begin{exampleblock}{\ldots gets translated to:}
        \begin{lstlisting}
double sqr(double x, double y) {
    return x * x + y * y;
}

kernel void vexcl_vector_kernel(
    ulong n,
    global double * prm_1,
    global double * prm_2,
    global double * prm_3
)
{
    for(size_t idx = get_global_id(0); idx < n; idx += get_global_size(0)) {
        prm_1[idx] = sqrt( sqr( prm_2[idx], prm_3[idx] ) );
    }
}
        \end{lstlisting}
    \end{exampleblock}
    \begin{tikzpicture}[overlay,scale=0.6]
        \draw (16,8.75) node(sqrt)[draw,fill=white,ellipse,drop shadow]{sqrt};

        \draw (sqrt) +(0.00,-2) node(sqr)[draw,fill=white,ellipse,drop shadow]{sqr};

        \draw (sqr) +(-2.00,-1) node(x) [draw,fill=white,drop shadow,minimum size=0.5cm]{x};
        \draw (sqr) +( 2.20,-1) node(y) [draw,fill=white,drop shadow,minimum size=0.5cm]{y};

        \draw (sqrt) -- (sqr);
        \draw (sqr)  -- (x);
        \draw (sqr)  -- (y);
    \end{tikzpicture}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Functions may be not only convenient, but also effective}
    \begin{exampleblock}{Same example without using a function:}
        \begin{lstlisting}
Z = sqrt( X * X + Y * Y );
        \end{lstlisting}
    \end{exampleblock}
    \begin{exampleblock}{\ldots gets translated to:}
        \begin{lstlisting}
kernel void vexcl_vector_kernel(
  ulong n,
  global double * prm_1,
  global double * prm_2,
  global double * prm_3,
  global double * prm_4,
  global double * prm_5
)
{
  for(size_t idx = get_global_id(0); idx < n; idx += get_global_size(0)) {
    prm_1[idx] = sqrt( ( ( prm_2[idx] * prm_3[idx] ) + ( prm_4[idx] * prm_5[idx] ) ) );
  }
}
        \end{lstlisting}
    \end{exampleblock}
    \begin{tikzpicture}[overlay,scale=0.6]
        \draw (16,8.75) node(sqrt)[draw,fill=white,ellipse,drop shadow]{sqrt};

        \draw (sqrt) +(0.00,-2) node(sum)[draw,fill=white,ellipse,drop shadow]{$+$};

        \draw (sum) +(-3.00,-1) node(mul1)[draw,fill=white,drop shadow,ellipse]{$*$};
        \draw (sum) +( 3.00,-1) node(mul2)[draw,fill=white,drop shadow,ellipse]{$*$};

        \draw (mul1) +(-2.00,-1) node(x1) [draw,fill=white,drop shadow,minimum size=0.5cm]{x};
        \draw (mul1) +( 2.00,-1) node(x2) [draw,fill=white,drop shadow,minimum size=0.5cm]{x};

        \draw (mul2) +(-2.00,-1) node(y1) [draw,fill=white,drop shadow,minimum size=0.5cm]{y};
        \draw (mul2) +( 2.00,-1) node(y2) [draw,fill=white,drop shadow,minimum size=0.5cm]{y};

        \draw (sqrt) -- (sum);
        \draw (sum)  -- (mul1);
        \draw (sum)  -- (mul2);
        \draw (mul1) -- (x1);
        \draw (mul1) -- (x2);
        \draw (mul2) -- (y1);
        \draw (mul2) -- (y2);
    \end{tikzpicture}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Tagged terminals}
    \begin{itemize}
        \item Programmer may help VexCL to recognize same terminals by
            tagging them:
    \end{itemize}
    \begin{columns}
        \begin{column}{0.46\textwidth}
            \begin{exampleblock}{Like this:}
                \begin{lstlisting}
using vex::tag;
Z = sqrt(tag<1>(X) * tag<1>(X) +
          tag<2>(Y) * tag<2>(Y));
                \end{lstlisting}
            \end{exampleblock}
        \end{column}
        \begin{column}{0.46\textwidth}
            \begin{exampleblock}{or, equivalently:}
                \begin{lstlisting}
auto x = tag<1>(X);
auto y = tag<2>(Y);
Z = sqrt(x * x + y * y);
                \end{lstlisting}
            \end{exampleblock}
        \end{column}
    \end{columns}
    \begin{exampleblock}{}
        \begin{lstlisting}
kernel void vexcl_vector_kernel(
  ulong n,
  global double * prm_1,
  global double * prm_2,
  global double * prm_3
)
{
  for(size_t idx = get_global_id(0); idx < n; idx += get_global_size(0)) {
    prm_1[idx] = sqrt( ( ( prm_2[idx] * prm_2[idx] ) + ( prm_3[idx] * prm_3[idx] ) ) );
  }
}
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Reusing intermediate results}
    \begin{columns}
        \begin{column}{0.48\textwidth}
            \begin{itemize}
                \item Some expressions may have several inclusions of the
                    same subexpression:
            \end{itemize}
            \begin{exampleblock}{}
                \begin{lstlisting}
Z = log(X) * (log(X) + Y);
                \end{lstlisting}
            \end{exampleblock}
            \begin{itemize}
                \item \code{log(X)} will be computed twice here.
                \item One could tag \code{X} and hope that the compiler is
                    smart enough\ldots
            \end{itemize}
        \end{column}
        \begin{column}{0.48\textwidth}
            \begin{tikzpicture}
                \draw (0,0) node(mul)[draw,fill=white,ellipse,drop shadow,minimum size=0.8cm]{$*$};
                \draw (mul)  +( 1.50,-1) node(sum)  [draw,fill=white,ellipse,drop shadow,minimum size=0.8cm]{$+$};
                \draw (mul)  +(-1.50,-1) node(log1) [draw,fill=chameleon2!50,ellipse,drop shadow,minimum size=0.5cm]{log};
                \draw (sum)  +(-1.50,-1) node(log2) [draw,fill=chameleon2!50,ellipse,drop shadow,minimum size=0.5cm]{log};
                \draw (log1) +( 0.00,-1) node(x1)   [draw,fill=chameleon2!50,drop shadow,minimum size=0.5cm]{x};
                \draw (log2) +( 0.00,-1) node(x2)   [draw,fill=chameleon2!50,drop shadow,minimum size=0.5cm]{x};
                \draw (sum)  +( 1.50,-1) node(y)    [draw,fill=white,drop shadow,minimum size=0.5cm]{y};

                \draw (mul)  -- (log1);
                \draw (mul)  -- (sum);
                \draw (log1) -- (x1);
                \draw (sum)  -- (log2);
                \draw (sum)  -- (y);
                \draw (log2) -- (x2);
            \end{tikzpicture}
        \end{column}
    \end{columns}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Temporaries}
    \begin{itemize}
        \item But it is also possible to introduce a temporary variable
            explicitly:
    \end{itemize}
    \begin{exampleblock}{}
        \begin{lstlisting}
auto tmp = vex::make_temp<1>( log(X) );
Z = tmp * (tmp + Y);
        \end{lstlisting}
    \end{exampleblock}
    \begin{exampleblock}{}
        \begin{lstlisting}
kernel void vexcl_vector_kernel(
    ulong n,
    global double * prm_1,
    global double * prm_2,
    global double * prm_3
)
{
    for(size_t idx = get_global_id(0); idx < n; idx += get_global_size(0)) {
        double temp_1 = log( prm_2[idx] );
        prm_1[idx] = ( temp_1 * ( temp_1 + prm_3[idx] ) );
    }
}
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Permutations} \singledevice
    \begin{itemize}
        \item \code{vex::permutation(expr)} takes arbitrary integral
            valued expression\\
            and returns a functor:
            \pause
            \begin{exampleblock}{}
                \begin{lstlisting}
auto reverse = vex::permutation(N - 1 - vex::element_index());
y = reverse(x);
                \end{lstlisting}
            \end{exampleblock}
            \pause
        \item Permutations are writable:
            \begin{exampleblock}{}
                \begin{lstlisting}
reverse(y) = x;
                \end{lstlisting}
            \end{exampleblock}
    \end{itemize}
\end{frame}

\note{ }

\subsection{Slicing}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Slicing} \singledevice
    \begin{itemize}
        \item When working with dense multidimensional matrices, it is general
            practice to store those in continuous 1D arrays.
            \begin{itemize}
                \item An instance of \code{vex::slicer<NDIM>} class allows to
                    access sub-blocks of such matrix.
            \end{itemize}
    \end{itemize}
    \begin{exampleblock}{n-by-n matrix and a slicer:}
        \begin{lstlisting}
vex::vector<double> x(ctx, n * n);
vex::slicer<2> slice(vex::extents[n][n]); // Can be used with any vector of appropriate size
        \end{lstlisting}
    \end{exampleblock}
    \pause
    \begin{exampleblock}{Access row or column of the matrix:}
        \begin{lstlisting}[firstnumber=last]
using vex::_;
y = slice[42](x);       // 42nd row
y = slice[_][42](x);    // 42nd column
slice[_][10](x) = y;    // Slices are writable
        \end{lstlisting}
    \end{exampleblock}
    \pause
    \begin{exampleblock}{Use ranges to select sub-blocks:}
        \begin{lstlisting}[firstnumber=last]
z = slice[vex::range(0, 2, n)][vex::range(10, 20)](x);
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Random number generation}
    \begin{itemize}
        \item VexCL provides\footnote{Contributed by
            \href{https://github.com/neapel}{Pascal Germroth}
            $\langle$\href{mailto:pascal@ensieve.org}{pascal@ensieve.org}$\rangle$}
            \emph{counter-based} random number generators from
            Random123\footnote{D. E. Shaw Research,
                \href{http://www.deshawresearch.com/resources\_random123.html}{http://www.deshawresearch.com/resources\_random123.html}}
            suite.
            \vspace{-0.5\baselineskip}
            \begin{itemize}
                \item The generators are \emph{stateless}; mixing functions are
                    applied to element indices.
                \item Implemented families: \code{threefry} and \code{philox}.
                \item Both pass TestU01/BigCrush; up to \alert{$2^{64}$}
                    independent streams with a period of \alert{$2^{128}$}.
                \item Performance: \alert{$\approx 10^{10}$}~Samples/sec (Tesla
                    K20c).
            \end{itemize}
        \item \code{vex::Random<T,G>}~--- uniform distribution.
        \item \code{vex::RandomNormal<T,G>}~--- normal distribution.
    \end{itemize}
    \begin{exampleblock}{}
        \begin{lstlisting}
vex::Random<double> rnd;
vex::vector<double> x(ctx, n);

x = rnd(vex::element_index(), std::rand());
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note[itemize]{
\item Random number generation is a useful feature that is used often in, e.g.,
    molecular dynamics.
\item Random number generators in VexCL are stateless, so they don't require
    additional storage or global memory interactions. Randomness is obtained by
    applying mixing functions to element indices.
}

\subsection{Reductions}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Reductions}
    \begin{itemize}
        \item Class \code{vex::Reductor<T, kind>} allows to reduce arbitrary
            \emph{vector expression} to a\\ single value of type \code{T}.
        \item Supported reduction kinds: \code{SUM}, \code{MIN}, \code{MAX}
    \end{itemize}
    \begin{exampleblock}{Inner product}
        \begin{lstlisting}
vex::Reductor<double, vex::SUM> sum(ctx);
double s = sum(x * y);
        \end{lstlisting}
    \end{exampleblock}
    \begin{exampleblock}{Number of elements in x between 0 and 1}
        \begin{lstlisting}
vex::Reductor<size_t, vex::SUM> sum(ctx);
size_t n = sum( (x > 0) && (x < 1) );
        \end{lstlisting}
    \end{exampleblock}
    \begin{exampleblock}{Maximum distance from origin}
        \begin{lstlisting}
vex::Reductor<double, vex::MAX> max(ctx);
double d = max( sqrt(x * x + y * y) );
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note[itemize]{
\item Reduction is an operation of reducing a vector to a single value.
\item The most frequent types are summation and finding minimum or maximum
    element of a vector.
\item VexCL provides Reductor functor that accepts any valid vector expression
    as a parameter.
\item For example, to compute an inner product of two vectors we compute sum of
    their elementwise product.
\item To find number of elements in vector x that are greater than zero and
    less than one, we compute sum of the corresponding boolean expression.
\item And to find the maximum distance from axis origin for a set of
    two-dimensional points, we compute exactly that: max of their radius.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Monte Carlo $\pi$}
    \vspace{-1\baselineskip}
    \begin{columns}
        \begin{column}{0.55\textwidth}
            \begin{itemize}
                \item Compute approximate value of $\pi$:
            \end{itemize}
            \vspace{\baselineskip}
            \begin{equation*}
                \frac{\text{area of circle}}{\text{area of square}} =
                \frac{\pi r^2}{(2r)^2} = \frac{\pi}{4},
            \end{equation*}
            \begin{equation*}
                \pi = 4 \frac{\text{area of circle}}{\text{area of square}}
                \approx 4 \frac{\text{\color{chameleon3}{points in
                circle}}}{\text{\color{chameleon3}{all}
                \color{chameleon2}{points}}}
            \end{equation*}
        \end{column}
        \begin{column}{0.35\textwidth}
            \begin{figure}
                \includegraphics[width=\textwidth]{mcpi}
            \end{figure}
        \end{column}
    \end{columns}
    \begin{exampleblock}{}
        \begin{lstlisting}[texcl=true]
vex::Random<cl_double2> rnd; // Generates 2D points in $[0,1]\times[0,1]$
vex::Reductor<size_t, vex::SUM> sum(ctx);

double pi = 4.0 * sum( length( rnd(vex::element_index(0, n), seed) ) < 1 ) / n;
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note[itemize]{
\item Here is a bit more complex example of what you can do with VexCL.
\item Imagine we want to compute an approximate value of $\pi$ with Monte-Carlo
    method. We can use the following equalities to do this.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Sparse matrix~-- vector products} \additive
    \begin{itemize}
        \item Class \code{vex::SpMat<T>} holds representation of a sparse
            matrix on compute devices.
        \item Constructor accepts matrix in common CRS format:
            \begin{itemize}
                \item row indices, columns and values of nonzero entries.
            \end{itemize}
    \end{itemize}
    \begin{exampleblock}{Construct matrix}
        \begin{lstlisting}
vex::SpMat<double> A(ctx, n, n, row.data(), col.data(), val.data());
        \end{lstlisting}
    \end{exampleblock}

    \begin{exampleblock}{Compute residual value}
        \begin{lstlisting}[firstnumber=last]
// vex::vector<double> u, f, r;
r = f - A * u;
double res = max( fabs(r) );
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note[itemize]{
\item Sparse matrix -- vector operation is also provided. A matrix is imported
    from commonly used compressed row storage format.
\item Note that matrix-vector product is not a first-class citizen in vector
    expressions. It uses neighbor values; and neighbors may reside on a
    different compute device. So extra work is needed to exchange data between
    devices. That is why matrix-vector products may only be used in additive
    expressions.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Inlining sparse matrix~-- vector products}
    \singledevice

    \begin{itemize}
        \item SpMV may only be used in additive expressions:
            \begin{itemize}
                \item Needs data exchange between compute devices.
                \item Impossible to implement with single kernel.
            \end{itemize}
        \item This restriction may be lifted for single-device contexts:
    \end{itemize}
    \begin{exampleblock}{}
        \begin{lstlisting}[numbers=none]
r = f - vex::make_inline(A * u);
double res = max( fabs(r) );

// Do not store intermediate results:
res = max( fabs( f - vex::make_inline(A * u) ) );
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Raw pointer arithmetic} \singledevice
    \begin{itemize}
        \item \code{raw_pointer(const vector<T>&)} function returns pointer to
            vector's data\\ inside vector expression.
            \begin{itemize}
                \item May be used to implement complex access patterns.
            \end{itemize}
    \end{itemize}
    \begin{exampleblock}{1D Laplace operator:}
        \begin{lstlisting}
auto ptr   = vex::raw_pointer(x);
auto idx   = vex::element_index();

auto left  = if_else(idx > 0,     i - 1, i);
auto right = if_else(idx + 1 < n, i + 1, i);

y = 2 * ptr[idx] - ptr[left] - ptr[right];
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{N-body problem with raw pointers}
    \begin{equation*}
        y_i = \sum_{j \neq i} e^{-|x_i - x_j|}
    \end{equation*}
    \begin{exampleblock}{}
        \begin{lstlisting}
VEX_FUNCTION(double, nbody, (size_t, n)(size_t, i)(double*, x),
    double sum = 0, myval = x[i];
    for(size_t j = 0; j < n; ++j)
        if (j != i) sum += exp(-fabs(x[j] - myval));
    return sum;
    );

y = nbody(x.size(), vex::element_index(), raw_pointer(x));
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note{ }

\subsection{Fast Fourier Transform}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Fast Fourier Transform \singledevice}
    \begin{itemize}
        \item VexCL provides FFT implementation\footnote{Contributed by
            \href{https://github.com/neapel}{Pascal Germroth}
            $\langle$\href{mailto:pascal@ensieve.org}{pascal@ensieve.org}$\rangle$}:
            \begin{itemize}
                \item Arbitrary vector expressions as input
                \item Multidimensional transforms
                \item Arbitrary sizes
            \end{itemize}
    \end{itemize}
    \begin{exampleblock}{Solve Poisson equation with FFT:}
        \begin{lstlisting}
vex::FFT<double, cl_double2> fft(ctx, n);
vex::FFT<cl_double2, double> ifft(ctx, n, vex::inverse);

vex::vector<double> rhs(ctx, n), u(ctx, n), K(ctx, n);
// ... initialize vectors ...

u = ifft( K * fft(rhs) );
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Summary}
    \begin{itemize}
        \item VexCL allows to write compact and readable code.
            \begin{itemize}
                \item Its great for fast prototyping of scientific GPGPU
                    applications.
                \item The performance is often on-par with manually written
                    kernels.
            \end{itemize}
    \end{itemize}
    \vspace{\baselineskip}
    \begin{itemize}
        \item[{[1]}] D. Demidov, K. Ahnert, K. Rupp, and P. Gottschling.\\
            Programming CUDA and OpenCL: A Case Study Using Modern \CXX Libraries.\\
            \emph{SIAM J. Sci. Comput.,} 35(5):C453  C472, 2013.\\
            \href{http://dx.doi.org/10.1137/120903683}{doi:10.1137/120903683}
        \item[{[2]}] K. Ahnert, D. Demidov, and M. Mulansky.\\
            Solving ordinary differential equations on GPUs.\\
            In \emph{Numerical Computations with GPUs} (pp. 125-157).  Springer, 2014.
            \href{http://dx.doi.org/10.1007/978-3-319-06548-9\_7}{doi:10.1007/978-3-319-06548-9\_7}
    \end{itemize}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}
    \begin{center}
        \huge{How does it work?}
    \end{center}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\section{Expression templates}
\begin{frame}
    \sectionpage
\end{frame}

\note{}

%----------------------------------------------------------------------------
\begin{frame}{Expression templates}
    \begin{itemize}
        \item How to implement a DSL in \CXX \emph{effectively}?
            \vspace{\baselineskip}
        \item The idea is quite old:
            \begin{description}[1995:]
                \item[1995:] \emph{Todd Veldhuizen}, Expression templates,
                    \CXX Report.
                \item[1996:]
                    Blitz{\raisebox{.2ex}{\relsize{-3}\bf ++}}\xspace is a \CXX
                    class library for scientific computing\\
                    \emph{which provides performance on par with Fortran
                    77/90}.
            \end{description}
        \item Today:
            \begin{itemize}
                \item Armadillo, Blaze, Boost.uBLAS, Eigen, MTL, etc.
            \end{itemize}
            \vspace{\baselineskip}
        \item \alert{\emph{But how does it work?}}
    \end{itemize}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Simple example: Vector arithmetic}
    \begin{exampleblock}{We want to be able to write:}
        \begin{lstlisting}
x = y + z;
        \end{lstlisting}
    \end{exampleblock}

    \begin{exampleblock}{and it has to be as effective as:}
        \begin{lstlisting}
for(size_t i = 0; i < n; ++i)
    x[i] = y[i] + z[i];
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{\CXX allows us to overload operators!}
    \begin{exampleblock}{}
        \begin{lstlisting}
vector operator+(const vector &a, const vector &b) {
    vector tmp( a.size() );
    for(size_t i = 0; i < a.size(); ++i)
        tmp[i] = a[i] + b[i];
    return tmp;
}
        \end{lstlisting}
    \end{exampleblock}
    \begin{itemize}
        \item Any problems?
            \begin{itemize}
                \item<2> Extra memory allocation
                \item<2> Extra memory I/O
            \end{itemize}
    \end{itemize}
    \begin{uncoverenv}<2>
        \begin{columns}
            \begin{column}{0.45\textwidth}
                \begin{exampleblock}{}
                    \begin{lstlisting}[aboveskip=0.4\baselineskip,belowskip=0.4\baselineskip]
a = x + y + z;

                    \end{lstlisting}
                \end{exampleblock}
                \begin{itemize}
                    \item 2 temporary vectors
                    \item $8 \times \text{n}$ memory reads/writes
                \end{itemize}
            \end{column}
            \begin{column}{0.45\textwidth}
                \begin{exampleblock}{}
                    \begin{lstlisting}
for(size_t i = 0; i < n; ++i)
    a[i] = x[i] + y[i] + z[i];
                    \end{lstlisting}
                \end{exampleblock}
                \begin{itemize}
                    \item no temporaries
                    \item $4 \times \text{n}$ memory reads/writes
                \end{itemize}
            \end{column}
        \end{columns}
    \end{uncoverenv}
\end{frame}

\note{
    Sometimes obvious solutions are the right ones\ldots
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Lazy evaluation v0.1}
    \begin{description}
        \item[The idea:] postpone the actual evaluation until assignment.
    \end{description}
    \pause
    \begin{exampleblock}{}
        \begin{lstlisting}
struct vsum {
    const vector &lhs;
    const vector &rhs;
};
        \end{lstlisting}
        \pause
        \begin{lstlisting}[firstnumber=last]

vsum operator+(const vector &a, const vector &b) {
    return vsum{a, b};
}
        \end{lstlisting}
        \pause
        \begin{lstlisting}[firstnumber=last]

const vector& vector::operator=(const vsum &s) {
    for(size_t i = 0; i < n; ++i)
        data[i] = s.lhs[i] + s.rhs[i];
    return *this;
}
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Problem: its not composable}
    \begin{exampleblock}{What will happen here?}
        \begin{lstlisting}
a = x + y + z;
        \end{lstlisting}
    \end{exampleblock}

    \begin{exampleblock}<2>{}
        \begin{verbatim}
lazy_v1.cpp:38:15: error: invalid operands to binary expression
      ('vsum' and 'vector')
    a = x + y + z;
        ~~~~~ ^ ~
lazy_v1.cpp:12:12: note: candidate function not viable:
      no known conversion from 'vsum' to 'const vector' for 1st argument
vsum operator+(const vector &a, const vector &b) {
     ^
1 error generated.
        \end{verbatim}
    \end{exampleblock}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile,shrink=2]{Lazy evaluation v0.2}
    \begin{exampleblock}{}
        \begin{lstlisting}
template <class LHS, class RHS>
struct vsum {
    const LHS &lhs;
    const RHS &rhs;

    double operator[](size_t i) const { return lhs[i] + rhs[i]; }
};
        \end{lstlisting}
        \pause
        \begin{lstlisting}[firstnumber=last]

template <class LHS, class RHS>
vsum<LHS, RHS> operator+(const LHS &a, const RHS &b) {
    return vsum<LHS, RHS>{a, b};
}
        \end{lstlisting}
        \pause
        \begin{lstlisting}[firstnumber=last]

template<class Expr>
const vector& vector::operator=(const Expr &expr) {
    for(int i = 0; i < n; ++i) data[i] = expr[i];
    return *this;
}
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}{Problem: its not general enough}
    \begin{figure}
        \begin{tikzpicture}
            \draw ( 0.0, 0.0) node{\includegraphics[width=0.75\textwidth]{times_in_life}};
            \draw (-4.2,-3.4) node{\href{http://www.youtube.com/watch?v=j4o2PDwKdcA}{\includegraphics[width=0.15\textwidth]{like}}};
            \draw ( 1.8, 0.8) node{There are times in life};
            \draw ( 1.8,-0.1) node{\Large when addition alone};
            \draw ( 1.8,-1.2) node{\Huge is not enough};
        \end{tikzpicture}
    \end{figure}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Lazy evaluation v0.3}
    \begin{exampleblock}{}
        \begin{lstlisting}
struct plus {
    static double apply(double a, double b) { return a + b; }
};
        \end{lstlisting}
        \pause
        \begin{lstlisting}[firstnumber=last]

template <class LHS, class OP, class RHS>
struct binary_op {
    const LHS &lhs;
    const RHS &rhs;

    double operator[](size_t i) const { return OP::apply(lhs[i], rhs[i]); }
};
        \end{lstlisting}
        \pause
        \begin{lstlisting}[firstnumber=last]

template <class LHS, class RHS>
binary_op<LHS, plus, RHS> operator+(const LHS &a, const RHS &b) {
    return binary_op<LHS, plus, RHS>{a, b};
}
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Expression templates are trees}
    \begin{columns}
        \begin{column}{0.4\textwidth}
            \begin{exampleblock}{The expression in the RHS of:}
                \begin{onlyenv}<1>
                    \begin{lstlisting}
a = x + y;
                    \end{lstlisting}
                \end{onlyenv}
                \begin{onlyenv}<2->
                    \begin{lstlisting}
a = x + y - z;
                    \end{lstlisting}
                \end{onlyenv}
            \end{exampleblock}
            \begin{exampleblock}{... is of type:}
                \begin{uncoverenv}<2->
                    \begin{lstlisting}[numbers=none]
binary_op<
                    \end{lstlisting}
                \end{uncoverenv}
                \begin{lstlisting}[numbers=none]
    binary_op<
        vector,
        plus,
        vector
    >
                \end{lstlisting}
                \begin{uncoverenv}<2->
                    \begin{lstlisting}[numbers=none]
    , minus
    , vector
>
                    \end{lstlisting}
                \end{uncoverenv}
            \end{exampleblock}
        \end{column}
        \begin{column}{0.55\textwidth}
            \begin{figure}
                \begin{tikzpicture}
                    \uncover<2->{
                    \draw (0,0) node(sub)            [treenode,circle]{$-$};
                    \draw (sub) +( 1.50,-1) node(z)  [treenode,minimum size=0.5cm]{z};
                    }
                    \draw (sub) +(-1.50,-1) node(add)[treenode,circle]{$+$};
                    \draw (add) +(-1.50,-1) node(x)  [treenode,minimum size=0.5cm]{x};
                    \draw (add) +( 1.50,-1) node(y)  [treenode,minimum size=0.5cm]{y};

                    \uncover<2->{
                    \draw (sub) -- (add);
                    \draw (sub) -- (z);
                    }
                    \draw (add) -- (x);
                    \draw (add) -- (y);

                    \uncover<6->{
                        \draw (sub) +(-2.5, 1.2) node[anchor=west]{\code{#pragma omp parallel for}};
                    }
                    \uncover<3->{
                        \draw (sub) +(-2.5, 0.7) node[anchor=west]{\code{for(size_t i = 0; i < n; ++i)}};
                        \draw (sub) +(-1.8, 0.0) node[anchor=west]{\code{a[i] =}};
                    }
                    \uncover<3> {\draw (sub) +(0.7, 0) node{\code{[i]}};}
                    \uncover<4> {\draw (add) +(0.7, 0) node{\code{[i]}};}
                    \uncover<4->{\draw (z)   +(0.6, 0) node{\code{[i]}};}
                    \uncover<5->{\draw (x)   +(0.6, 0) node{\code{[i]}};}
                    \uncover<5->{\draw (y)   +(0.6, 0) node{\code{[i]}};}
                \end{tikzpicture}
            \end{figure}
            \begin{uncoverenv}<7>
                \begin{itemize}
                    \item The \CXX compiler walks the tree.
                    \item \code{binary_op::operator[]} does the actual work.
                \end{itemize}
            \end{uncoverenv}
        \end{column}
    \end{columns}
\end{frame}

\note{
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{So far, so good}
    \begin{exampleblock}{It is now possible to:}
        \begin{lstlisting}
v = a * x + b * y;

double c = (x + y)[42];
        \end{lstlisting}
    \end{exampleblock}

    \begin{exampleblock}{... and its as effective as:}
        \begin{lstlisting}
for(size_t i = 0; i < n; ++i)
    v[i] = a[i] * x[i] + b[i] * y[i];

double c = x[42] + y[42];
        \end{lstlisting}
    \end{exampleblock}
    \begin{itemize}
        \item No temporaries involved.
        \item Optimizing compiler is able to inline everything.
            \vspace{\baselineskip}
            \pause
        \item<alert@2> \emph{But how is that related to OpenCL code generation?}
    \end{itemize}
\end{frame}

\note{
    It turns out that once you understand how expression templates are working,
    it is really easy to shift to code generation.
}

%----------------------------------------------------------------------------
\section{OpenCL code generation}
\begin{frame}
    \sectionpage
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}{How does OpenCL work?}
    \begin{enumerate}
        \item A compute kernel is compiled at runtime from \CC source.
        \item Kernel parameters are set through API calls.
        \item Kernel is launched on a compute device.
    \end{enumerate}
    \vspace{\baselineskip}
    \pause
    \begin{itemize}
        \item Source may be read from a file, or stored in a static
            string, or \alert{\emph{generated}}.
    \end{itemize}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Generating kernel source from an expression}
    \begin{exampleblock}{We want this expression:}
        \begin{lstlisting}
a = x + y - z;
        \end{lstlisting}
    \end{exampleblock}
    \begin{exampleblock}{\ldots to result in this kernel:}
        \begin{lstlisting}
kernel void vexcl_vector_kernel(
    ulong n,
    global double * res,
    global double * prm1,
    global double * prm2,
    global double * prm3
)
{
    for(size_t idx = get_global_id(0); idx < n; idx += get_global_size(0)) {
        res[idx] = ( ( prm1[idx] + prm2[idx] ) - prm3[idx] );
    }
}
        \end{lstlisting}
    \end{exampleblock}
    \begin{tikzpicture}[overlay,scale=0.8]
        \draw (11,6) node(sub)[treenode,circle]{$-$};
        \draw (sub) +( 1.50,-1) node(z)  [treenode,minimum size=0.5cm]{z};
        \draw (sub) +(-1.50,-1) node(add)[treenode,circle]{$+$};
        \draw (add) +(-1.50,-1) node(x)  [treenode,minimum size=0.5cm]{x};
        \draw (add) +( 1.50,-1) node(y)  [treenode,minimum size=0.5cm]{y};
        \draw (sub) -- (add);
        \draw (sub) -- (z);
        \draw (add) -- (x);
        \draw (add) -- (y);
    \end{tikzpicture}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Declaring parameters}
    \begin{exampleblock}{Each terminal knows what parameters it needs:}
        \begin{lstlisting}
/*static*/ void vector::declare_params(std::ostream &src, unsigned &pos) {
    src << ",\n    global double * prm" << ++pos;
}
        \end{lstlisting}
    \end{exampleblock}
    \begin{exampleblock}{An expression just asks its terminals to do the work:}
        \begin{lstlisting}[firstnumber=last]
template <class LHS, class OP, class RHS>
/*static*/ void binary_op<LHS, OP, RHS>::declare_params(
                            std::ostream &src, unsigned &pos)
{
    LHS::declare_params(src, pos);
    RHS::declare_params(src, pos);
}
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Building string representation for expression}
    \begin{exampleblock}{}
        \begin{lstlisting}
struct plus {
    static std::string to_string(std::ostream &src) { src << " + "; }
};
        \end{lstlisting}
        \pause
        \begin{lstlisting}[firstnumber=last]

/*static*/ void vector::to_string(std::ostream &src, unsigned &pos) {
    src << "prm" << ++pos << "[idx]";
}
        \end{lstlisting}
        \pause
        \begin{lstlisting}[firstnumber=last]

template <class LHS, class OP, class RHS>
/*static*/ void binary_op<LHS, OP, RHS>::to_string(std::ostream &src, unsigned &pos) {
    src << "( ";
    LHS::to_string(src, pos);
    OP::to_string(src);
    RHS::to_string(src, pos);
    src << " )";
}
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note[itemize]{
\item The obvious observation is that\ldots
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Generating kernel source}
    \setbeamercovered{transparent=40}
    \begin{exampleblock}{}
        \begin{uncoverenv}<1>
            \begin{lstlisting}
template <class LHS, class RHS>
std::string kernel_source() {
    std::ostringstream src;

    src << "kernel void vexcl_vector_kernel(\n    ulong n";
            \end{lstlisting}
        \end{uncoverenv}
        \begin{uncoverenv}<1,2>
            \begin{lstlisting}[firstnumber=last]
    unsigned pos = 0;
    LHS::declare_params(src, pos);
    RHS::declare_params(src, pos);
            \end{lstlisting}
        \end{uncoverenv}
        \begin{uncoverenv}<1>
            \begin{lstlisting}[firstnumber=last]
    src << ")\n{\n"
            "  for(size_t idx = get_global_id(0); idx < n; idx += get_global_size(0)) {\n"
            "    ";
            \end{lstlisting}
        \end{uncoverenv}
        \begin{uncoverenv}<1,3>
            \begin{lstlisting}[firstnumber=last]
    pos = 0;
    LHS::to_string(src, pos); src << " = ";
    RHS::to_string(src, pos); src << ";\n";
            \end{lstlisting}
        \end{uncoverenv}
        \begin{uncoverenv}<1>
            \begin{lstlisting}[firstnumber=last]
    src << "  }\n}\n";

    return src.str();
}
            \end{lstlisting}
        \end{uncoverenv}
    \end{exampleblock}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Setting kernel arguments}
    \begin{exampleblock}{}
        \begin{lstlisting}
void vector::set_args(cl::Kernel &krn, unsigned &pos) {
    krn.setArg(pos++, buffer);
}

template <class LHS, class OP, class RHS>
void binary_op<LHS, OP, RHS>::set_args(cl::Kernel &krn, unsigned &pos) {
    lhs.set_args(krn, pos);
    rhs.set_args(krn, pos);
}
        \end{lstlisting}
    \end{exampleblock}

    \vspace{\baselineskip}

    \begin{itemize}
        \item The methods are not static anymore!
    \end{itemize}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Gluing it all together}
    \setbeamercovered{transparent=40}
    \begin{exampleblock}{}
        \begin{uncoverenv}<1>
            \begin{lstlisting}
template <class Expr>
const vector& vector::operator=(const Expr &expr) {
            \end{lstlisting}
        \end{uncoverenv}
        \begin{uncoverenv}<1,2>
            \begin{lstlisting}[firstnumber=last]
    static cl::Kernel krn = build_kernel(device, kernel_source<This, Expr>());
            \end{lstlisting}
        \end{uncoverenv}
        \begin{uncoverenv}<1>
            \begin{lstlisting}[firstnumber=last]

    unsigned pos = 0;

    krn.setArg(pos++, size);        // n
    krn.setArg(pos++, buffer);      // result
    expr.set_args(krn, pos);        // other parameters

    queue.enqueueNDRangeKernel(krn, cl::NullRange, buffer.size(), cl::NullRange);

    return *this;
}
            \end{lstlisting}
        \end{uncoverenv}
    \end{exampleblock}
    \setbeamercovered{transparent=0}
    \begin{uncoverenv}<2>
        \begin{itemize}
            \item Each kernel is uniquely identified by its expression type.
                \begin{itemize}
                    \item Hence, we may use local static variable to cache the
                        kernel.
                    \item Kernel is generated and compiled once, applied many
                        times.
                \end{itemize}
        \end{itemize}
    \end{uncoverenv}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{What you saw is not what you get}
    \begin{tikzpicture}[remember picture,overlay]
        \node[anchor=north east,yshift=4pt,xshift=4pt] at (current page.north east) {
            \includegraphics[width=3.6cm]{clippy}
        };
    \end{tikzpicture}
    \begin{itemize}
        \item The actual implementation is a bit more complicated:
            \begin{itemize}
                \item There are unary, binary, n-ary expressions.
                \item There are special terminals requiring either global or
                    local preambles.
                \item There are builtin and user-defined functions.
                \item \ldots
            \end{itemize}
        \item Boost.Proto is used to keep the code mess contained.
    \end{itemize}
\end{frame}

\note[itemize] {
\item You are welcome to look at the actual code for more details.
}

%----------------------------------------------------------------------------
\section{Conclusion}
\begin{frame}{Conclusion} \forkme
    \vspace{-1\baselineskip}
    \begin{columns}
        \begin{column}{0.55\textwidth}
            \begin{itemize}
                \item Advantages of code generation:
                    \vspace{0.5\baselineskip}
                    \begin{itemize}
                        \item You get the exact kernel you need.
                            \vspace{0.5\baselineskip}
                        \item Flexibility of generic \CXX code\\
                            with \CC backend.
                            \vspace{0.5\baselineskip}
                        \item There is no need to use\\
                            vendor-specific solutions.
                            \vspace{\baselineskip}
                        \item Performance!
                    \end{itemize}
            \end{itemize}
        \end{column}
        \begin{column}{0.45\textwidth}
            \vspace{-1\baselineskip}
            \begin{figure}
                \begin{center}
                \includegraphics[width=\textwidth]{perfplot}

                \vspace{\baselineskip}
                {\small \sl
                    Integrate large number of ODEs\\
                    on a GPU with Boost.odeint [2].
                }
            \end{center}
            \end{figure}
        \end{column}
    \end{columns}
\end{frame}

\note{}

\end{document}
