\documentclass[@BEAMER_OPTIONS@]{beamer}
    @USE_PGFPAGES@

    \usetheme[alternativetitlepage=true,titleline=true]{Torino}
    \setbeamertemplate{navigation symbols}{}
    \setbeamertemplate{note page}[plain]
    \setbeamertemplate{caption}{\insertcaption}

    \usepackage{graphicx}
    \usepackage{tabularx}
    \usepackage{subfigure}
    \usepackage{xspace}
    \usepackage{adjustbox}
    \usepackage{tikz}
    \usepackage{relsize}
    \usepackage{fancyvrb}
    \fvset{fontsize=\footnotesize}
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{}
    \usepgflibrary{arrows}
    \usetikzlibrary{shadows,decorations.pathreplacing,patterns,shapes}
    \tikzstyle{every picture}=[semithick,>=stealth,remember picture]
    \usepackage{inconsolata}
    \usepackage{listings}
    \lstset{
        language=C++,
        basicstyle=\footnotesize\ttfamily,
        keywordstyle=\color{chameleon4}\bfseries,
        commentstyle=\color{chameleon1}\it\ttfamily,
        stringstyle=\color{chameleon1},
        numbers=left,
        numberstyle=\tiny,
        aboveskip=-0.02\baselineskip,
        belowskip=-0.02\baselineskip,
        columns=flexible,
        extendedchars=false,
        showstringspaces=false,
        morekeywords={global,kernel,ulong,size_t,cl_uint,cl_platform_id}
        }
    \newcommand{\code}[1]{\lstinline|#1|}
    \protected\def\plusplus{{\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\relsize{-3}\bf ++}}\xspace}
    \newcommand{\CXX}{{\rm C}\plusplus}
    \newcommand{\CC}{{\rm C99}\xspace}
    \newcommand{\www}[1]{\href{#1}{#1}}

    \input{ribbon}
    \newcommand{\forkme}{\ribbon{east}{chameleon1}{white}{\href{https://github.com/ddemidov/vexcl}{Fork me on GitHub}}}
    \newcommand{\footnoteline}{\begin{beamercolorbox}[wd=\textwidth,ht=.1ex,dp=0ex]{linemid}\end{beamercolorbox}}
    \renewcommand{\footnoterule}{\footnoteline\kern 2pt}

    \tikzset{
        treenode/.style={
            draw,
            fill=white,
            blur shadow,
            shadow xshift=1pt,
            shadow yshift=-1pt,
            shadow blur radius=2pt,
            shadow opacity=40
            }
        }


    \title{VexCL}
    \subtitle{Rapid development of scientific code for GPUs}
    \author{Denis Demidov}
    \institute{Institute of System Analysis / Russian Academy of Sciences,\\
    Kazan Federal University
    }
    \date{March 2018,
    \vspace{-1.9\baselineskip}\raisebox{-0.8\baselineskip}{\includegraphics[height=2.5\baselineskip]{lpt.pdf}}}

\begin{document}

%----------------------------------------------------------------------------
\begin{frame}{}
    \titlepage
\end{frame}

%----------------------------------------------------------------------------
\begin{frame}{Why GPUs?}
    \begin{itemize}
        \item November 2017:
            \begin{description}
                \item[Top500] - 6 out of top 10 systems use accelerators/GPUs
                \item[Green500] - 6 out of top 10 systems use GPUs
            \end{description}
    \end{itemize}

    \vspace{\baselineskip}

    \begin{block}{PizDaint system (No 3 in Top500)}
        \begin{tabularx}{\textwidth}{X|XX}
            & CPU & GPU \\
            \hline
            Model & Intel Xeon E5-2690 v3 & NVIDIA Tesla P100 16GB \\
            Peak performance & 500 GFLOPS & 4.7 TFLOPS \\
            Memory bandwidth & 68 GB/s & 732 GB/s \\
            % https://www.thinkmate.com/inside/promo/save-big-tesla-gpus
            Price & \$2000 & \$5700 \\
            Max power consumption & 135 W & 250 W \\
        \end{tabularx}
    \end{block}
\end{frame}

%----------------------------------------------------------------------------
\begin{frame}{Peak performance for CPUs and GPUs%
    \footnote{\www{https://www.karlrupp.net/2013/06/cpu-gpu-and-mic-hardware-characteristics-over-time/}}}
    \begin{figure}
        \includegraphics[height=0.8\textheight]{gflops-dp.png}
    \end{figure}
\end{frame}

%----------------------------------------------------------------------------
\begin{frame}{Memory bandwidth for CPUs and GPUs%
    \footnote{\www{https://www.karlrupp.net/2013/06/cpu-gpu-and-mic-hardware-characteristics-over-time/}}}
    \begin{figure}
        \includegraphics[height=0.8\textheight]{mem-bw.png}
    \end{figure}
\end{frame}

%----------------------------------------------------------------------------
\begin{frame}{VexCL~--- a vector expression template library for OpenCL/CUDA}
    \forkme

    \begin{itemize}
        \item Created for ease of \CXX based GPGPU development:
            \begin{itemize}
                \item Convenient notation for vector expressions
                \item OpenCL/CUDA JIT code generation
                \item Easily combined with existing libraries/code
                \item Header-only
            \end{itemize}
            \vspace{\baselineskip}
        \item Supported backends:
            \begin{itemize}
                \item OpenCL (Khronos \CXX bindings)
                \item OpenCL (Boost.Compute)
                \item NVIDIA CUDA
                \item JIT-compiled OpenMP
            \end{itemize}
            \vspace{\baselineskip}
        \item The source code is available under the MIT license:
            \begin{itemize}
                \item \href{https://github.com/ddemidov/vexcl}{https://github.com/ddemidov/vexcl}
            \end{itemize}
            \vspace{\baselineskip}
    \end{itemize}
\end{frame}

\note[itemize]{
\item VexCL is a vector expression template library for OpenCL. It allows you
    to use convenient matlab-like notation for vector operations and it
    generates the appropriate compute kernels for you automatically.
\item The library is header-only, so you don't have to build it to use it. The
    source code of the library is available on GitHub under very liberal
    MIT license.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Hello OpenCL}
    \begin{itemize}
        \item Compute sum of two vectors in parallel:
            \begin{itemize}
                \item \code{A} and \code{B} are large vectors.
                \item Compute \code{A = A + B}.
            \end{itemize}
            \vspace{\baselineskip}
        \item Overview of (any) OpenCL solution:
            \begin{itemize}
                \item Initialize OpenCL context
                \item Allocate memory
                \item Transfer input data
                \item Run computations
                \item Get the results
            \end{itemize}
    \end{itemize}
\end{frame}

\note[itemize]{
\item Let's start with a motivating example. This is classical hello world
    example for OpenCL: addition of two large vector.
\item To do anything with the OpenCL, you need to perform some standard steps,
    like context initialization, memory allocation and transfer, and the
    computations that you needed to do in the first place.
\item So let's look at how these steps are done with native OpenCL API.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Hello OpenCL}
    \setbeamercovered{transparent=40}
    \vspace{-1\baselineskip}
    \begin{columns}
        \begin{column}[c]{0.25\textwidth}
            \begin{exampleblock}{}
                \begin{adjustbox}{width=0.25\textwidth, height=\textheight, keepaspectratio}
                    \begin{minipage}{\textwidth}
                        \begin{uncoverenv}<1>
                            \lstinputlisting[numbers=none,linerange={1-9}]{code/hello-opencl.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1,2>
                            \lstinputlisting[numbers=none,linerange={10-14}]{code/hello-opencl.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1,3>
                            \lstinputlisting[numbers=none,linerange={15-33}]{code/hello-opencl.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1,4>
                            \lstinputlisting[numbers=none,linerange={34-42}]{code/hello-opencl.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1,5>
                            \lstinputlisting[numbers=none,linerange={43-62}]{code/hello-opencl.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1,6>
                            \lstinputlisting[numbers=none,linerange={63-68}]{code/hello-opencl.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1,7>
                            \lstinputlisting[numbers=none,linerange={69-71}]{code/hello-opencl.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1>
                            \lstinputlisting[numbers=none,linerange={72-72}]{code/hello-opencl.cpp}
                        \end{uncoverenv}
                    \end{minipage}
                \end{adjustbox}
            \end{exampleblock}
        \end{column}
        \begin{column}[c]{0.7\textwidth}
            \begin{onlyenv}<2>
                \begin{exampleblock}{1. Query platforms}
                    \begin{adjustbox}{width=0.75\textwidth, height=\textheight, keepaspectratio}
                        \begin{minipage}{\textwidth}
                            \lstinputlisting[firstnumber=10, linerange={10-14}]{code/hello-opencl.cpp}
                        \end{minipage}
                    \end{adjustbox}
                \end{exampleblock}
            \end{onlyenv}
            \begin{onlyenv}<3>
                \begin{exampleblock}{2. Get the first available GPU}
                    \begin{adjustbox}{width=0.75\textwidth, height=\textheight, keepaspectratio}
                        \begin{minipage}{\textwidth}
                            \lstinputlisting[firstnumber=16, linerange={16-33}]{code/hello-opencl.cpp}
                        \end{minipage}
                    \end{adjustbox}
                \end{exampleblock}
            \end{onlyenv}
            \begin{onlyenv}<4>
                \begin{exampleblock}{3. Prepare and copy input data}
                    \begin{adjustbox}{width=0.75\textwidth, height=\textheight, keepaspectratio}
                        \begin{minipage}{\textwidth}
                            \lstinputlisting[firstnumber=35, linerange={35-42}]{code/hello-opencl.cpp}
                        \end{minipage}
                    \end{adjustbox}
                \end{exampleblock}
            \end{onlyenv}
            \begin{onlyenv}<5>
                \begin{exampleblock}{4. Write and compile the compute kernel}
                    \begin{adjustbox}{width=0.75\textwidth, height=\textheight, keepaspectratio}
                        \begin{minipage}{\textwidth}
                            \lstinputlisting[firstnumber=44, linerange={44-62}]{code/hello-opencl.cpp}
                        \end{minipage}
                    \end{adjustbox}
                \end{exampleblock}
            \end{onlyenv}
            \begin{onlyenv}<6>
                \begin{exampleblock}{5. Set kernel arguments and launch the computation}
                    \begin{adjustbox}{width=0.75\textwidth, height=\textheight, keepaspectratio}
                        \begin{minipage}{\textwidth}
                            \lstinputlisting[firstnumber=64, linerange={64-68}]{code/hello-opencl.cpp}
                        \end{minipage}
                    \end{adjustbox}
                \end{exampleblock}
            \end{onlyenv}
            \begin{onlyenv}<7>
                \begin{exampleblock}{6. Get the results from the compute device}
                    \begin{adjustbox}{width=0.75\textwidth, height=\textheight, keepaspectratio}
                        \begin{minipage}{\textwidth}
                            \lstinputlisting[firstnumber=70, linerange={70-71}]{code/hello-opencl.cpp}
                        \end{minipage}
                    \end{adjustbox}
                \end{exampleblock}
            \end{onlyenv}
        \end{column}
    \end{columns}
\end{frame}

\note[itemize]{
\item Here is the simplest example of using vexcl: addition of two vectors on a
    gpu card.
\item The first line is the context initialization. We provide a device filter
    to the context constructor and get all compute devices that satisfy the
    filter. Here we filter by type and get all available GPUs.
\item Data allocation and transfer is also simplified. \code{vex::vector}
    constructor allocates memory on device and possibly transfers initial data
    as well. The parameters here are list of command queues and either size or
    input host vector.
\item Line ten does what's needs to be done here. This simple expression leads
    to automatic kernel generation and launch. And then we copy the results
    back to host and see what we got.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Hello VexCL}
    \vspace{-1\baselineskip}
    \begin{columns}
        \begin{column}[t]{0.5\textwidth}
            \begin{exampleblock}{hello.cpp}
                \begin{adjustbox}{width=0.9\textwidth, height=\textheight, keepaspectratio}
                    \begin{minipage}{\textwidth}
                        \lstinputlisting{code/hello-vexcl.cpp}
                    \end{minipage}
                \end{adjustbox}
            \end{exampleblock}
        \end{column}
        \begin{column}[t]{0.45\textwidth}
            \begin{exampleblock}{CMakeLists.txt}
                \begin{adjustbox}{width=0.9\textwidth, height=\textheight, keepaspectratio}
                    \begin{minipage}{\textwidth}
                        \begin{lstlisting}[language=sh,
                        morekeywords={cmake_minimum_required,project,find_package,add_subdirectory,add_executable,target_link_libraries}]
cmake_minimum_required(VERSION 3.1)
project(hello)

find_package(VexCL)

add_executable(hello hello.cpp)
target_link_libraries(hello VexCL::OpenCL)
                        \end{lstlisting}
                    \end{minipage}
                \end{adjustbox}
            \end{exampleblock}
            \begin{itemize}
                \item Use \code{VexCL::Compute}, \code{VexCL::CUDA}, or
                    \code{VexCL::JIT} for use with Boost.Compute, CUDA, or
                    OpenMP backend
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\note[itemize]{
\item Here is the simplest example of using vexcl: addition of two vectors on a
    gpu card.
\item The first line is the context initialization. We provide a device filter
    to the context constructor and get all compute devices that satisfy the
    filter. Here we filter by type and get all available GPUs.
\item Data allocation and transfer is also simplified. \code{vex::vector}
    constructor allocates memory on device and possibly transfers initial data
    as well. The parameters here are list of command queues and either size or
    input host vector.
\item Line ten does what's needs to be done here. This simple expression leads
    to automatic kernel generation and launch. And then we copy the results
    back to host and see what we got.
}

%----------------------------------------------------------------------------
\section{Interface}
\begin{frame}
    \sectionpage
\end{frame}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Initialization}
    \begin{itemize}
        \item Multi-device and multi-platform computations are supported.
            \begin{itemize}
                \item Some operations are restricted to single-device
                    contexts.
            \end{itemize}
        \item VexCL context is initialized from combination of device filters.
    \end{itemize}
    \vspace{-0.5\baselineskip}
    \begin{overlayarea}{\textwidth}{0.4\textheight}
    \begin{exampleblock}{Initialize VexCL context on selected devices}
        \begin{onlyenv}<1>
        \begin{lstlisting}
vex::Context ctx( vex::Filter::Any );
        \end{lstlisting}
        \end{onlyenv}
        \begin{onlyenv}<2|handout:0>
        \begin{lstlisting}
vex::Context ctx( vex::Filter::GPU );
        \end{lstlisting}
        \end{onlyenv}
        \begin{onlyenv}<3|handout:0>
        \begin{lstlisting}
vex::Context ctx(vex::Filter::Name("Intel"));
        \end{lstlisting}
        \end{onlyenv}
        \begin{onlyenv}<4|handout:0>
        \begin{lstlisting}
vex::Context ctx(vex::Filter::GPU && vex::Filter::Platform("AMD"));
        \end{lstlisting}
        \end{onlyenv}
    \end{exampleblock}
    \end{overlayarea}
    \begin{figure}
        \uncover<1,2>{
            \includegraphics[width=0.2\textwidth]{tesla.png}\quad
        }
        \uncover<1,2,4>{
            \includegraphics[width=0.2\textwidth]{radeon.png}\quad
        }
        \uncover<1,3>{
            \includegraphics[width=0.17\textwidth]{intel.png}
        }
    \end{figure}
\end{frame}

\note[itemize]{
\item VexCL can transparently work with several compute devices that are
    present on your system.
\item We initialize the VexCL context with a device filter. The device filter
    is a simple functor that acts on device reference and returns a boolean
    value. Several standard filters are provided and you can write your own
    filters.
\item Let's assume that we have an NVIDIA GPU, an AMD GPU, and an Intel CPU
    installed.
    \begin{enumerate}
        \item The standard 'All' Filter select any device available, so we end
            with three devices in our context.
        \item If we want to select only GPUs, then we can filter the devices by
            type.
        \item It is also possible to combine the device filters with logical
            operators.  Here we select a GPU that is provided by AMD OpenCL
            platform.
        \item And here is an example of a custom filter. Here it selects any
            device that has at least 4GB of memory.
    \end{enumerate}
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Memory and work splitting}
    \setbeamercovered{transparent=40}
    \begin{exampleblock}{}
        \begin{onlyenv}<1|handout:0>
        \begin{lstlisting}
vex::Context ctx( vex::Filter::Count(1) );
        \end{lstlisting}
        \end{onlyenv}
        \begin{onlyenv}<2|handout:0>
        \begin{lstlisting}
vex::Context ctx( vex::Filter::GPU );
        \end{lstlisting}
        \end{onlyenv}
        \begin{onlyenv}<3>
        \begin{lstlisting}
vex::Context ctx( vex::Filter::DoublePrecision );
        \end{lstlisting}
        \end{onlyenv}
        \begin{uncoverenv}<1>
        \begin{lstlisting}[firstnumber=last]

vex::vector<double> x(ctx, N);
vex::vector<double> y(ctx, N);

x = vex::element_index() * (1.0 / N);
y = sin(2 * x) + sqrt(1 - x * x);
        \end{lstlisting}
        \end{uncoverenv}
    \end{exampleblock}
    \setbeamercovered{invisible}
    \begin{figure}
        \begin{tikzpicture}
            \draw (0,2.5) rectangle +(8,0.1);
            \draw (0,2.5) grid[step=0.1] +(8,0.1);
            \draw (-0.3,2.6) node{x};

            \draw (0,2.0) rectangle +(8,0.1);
            \draw (0,2.0) grid[step=0.1] +(8,0.1);
            \draw (-0.3,2.1) node[anchor=center]{y};

            \uncover<1-3> {
            \draw (1,0.5) node{\includegraphics[width=0.2\textwidth]{tesla.png}};
            }

            \uncover<2-3> {
            \draw (4,0.5) node{\includegraphics[width=0.2\textwidth]{radeon.png}};
            }

            \uncover<3> {
            \draw (7.5,0.5) node{\includegraphics[width=0.17\textwidth]{intel.png}};
            }

            \uncover<1|handout:0> {
            \draw[->,chameleon1,style=dashed] (0,2.7) -- (0,1.8)
                .. controls +(east:0.5) and +(north west:0.5) ..
                (1.4,1.5);
            \draw[->,chameleon1,style=dashed] (8,2.7) -- (8,1.8)
                .. controls +(west:0.5) and +(north east:0.5) ..
                (1.6,1.5);
            }

            \uncover<2|handout:0> {
            \draw[->,chameleon1,style=dashed] (0,2.7) -- (0,1.8)
                .. controls +(east:0.5) and +(north west:0.5) ..
                (1.4,1.5);
            \draw[->,chameleon1,style=dashed] (4,2.7) -- (4,1.8)
                .. controls +(west:0.5) and +(north east:0.5) ..
                (1.6,1.5);

            \draw[->,chameleon1,style=dashed] (4,2.7) -- (4,1.8)
                .. controls +(east:0.1) and +(north west:0.2) ..
                (4.4,1.5);
            \draw[->,chameleon1,style=dashed] (8,2.7) -- (8,1.8)
                .. controls +(west:0.5) and +(north east:0.5) ..
                (4.6,1.5);
            }

            \uncover<3> {
            \draw[->,chameleon1,style=dashed] (0,2.7) -- (0,1.8)
                .. controls +(east:0.5) and +(north west:0.5) ..
                (1.4,1.5);
            \draw[->,chameleon1,style=dashed] (3,2.7) -- (3,1.8)
                .. controls +(west:0.5) and +(north east:0.5) ..
                (1.6,1.5);

            \draw[->,chameleon1,style=dashed] (3,2.7) -- (3,1.8)
                .. controls +(east:0.5) and +(north west:0.2) ..
                (4.4,1.5);
            \draw[->,chameleon1,style=dashed] (6,2.7) -- (6,1.8)
                .. controls +(west:0.5) and +(north east:0.5) ..
                (4.6,1.5);

            \draw[->,chameleon1,style=dashed] (6,2.7) -- (6,1.8) -- (7.4,1.5);
            \draw[->,chameleon1,style=dashed] (8,2.7) -- (8,1.8) -- (7.6,1.5);
            }
        \end{tikzpicture}
    \end{figure}
\end{frame}

\note[itemize]{
\item Now that we know how to initialize VexCL context, let's see how device
    vectors are allocated.
\item Here we allocate three vectors, and initialize two of them with
    constant values.
\item Each vector receives a list of queues at initialization.  Since each
    queue corresponds to a specific device, vectors know where to put their
    data to.
    \begin{enumerate}
        \item For example, if we only have the Tesla card in our context, then
            it will hold the complete memory for all of our vectors.
        \item If we use both of the available GPUs, then the vectors will be
            split between the devices. This split is by default proportional to
            the GPU bandwidth and is guaranteed to be consistent for vectors of
            the same size. This consistency allows VexCL to run computations
            independently on all devices in context.
        \item If we add the CPU to the context, it will get smaller share of
            the data and arithmetic operations.
    \end{enumerate}
\item Care must be taken with the use of several devices. VexCL tries to split
    the memory as fair as it can, but it is probable that your program will
    run at the speed of the slowest device.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Copies between host and device memory}
    \begin{exampleblock}{}
        \begin{lstlisting}
vex::vector<double> d(ctx, n);
std::vector<double> h(n);
double a[100];
        \end{lstlisting}
    \end{exampleblock}
    \vspace{\baselineskip}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{exampleblock}{STL-like range copies}
                \begin{lstlisting}
vex::copy(d.begin(), d.end(), h.begin());
vex::copy(d.begin(), d.begin() + 100, a);
                \end{lstlisting}
            \end{exampleblock}
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{exampleblock}{Simple copies}
                \begin{lstlisting}
vex::copy(d, h);
vex::copy(h, d);
                \end{lstlisting}
            \end{exampleblock}
        \end{column}
    \end{columns}
    \vspace{\baselineskip}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{exampleblock}{Map OpenCL buffer to host pointer}
                \begin{lstlisting}
auto p = d.map(devnum);
std::sort(&p[0], &p[d.part_size(devnum)]);
                \end{lstlisting}
            \end{exampleblock}
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{exampleblock}{Access single element (\emph{slow})}
                \begin{lstlisting}
double v = d[42];
d[0] = 0;
                \end{lstlisting}
            \end{exampleblock}
        \end{column}
    \end{columns}
\end{frame}

\note[itemize]{
\item Copies between host and device memory may be done with simple copy
    function that copy the complete vector either way,
\item or, if you need to do partial copy, you can use STL-like syntax.
\item Vectors also overload array subscript operator, so you can have direct
    read or write access to any element of a vector. But this should be used
    with caution because it is slow. The intended use for this is a single
    element access or debugging.
\item Data may also be accessed through iterators, so it is possible to use,
    for example, an STL algorithm with device vector as a temporary solution.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Expressions are translated into compute kernels}
    \begin{columns}
        \begin{column}{0.38\textwidth}
            \begin{exampleblock}{This expression:}
                \begin{lstlisting}
x = 2 * y - sin(z);
                \end{lstlisting}
            \end{exampleblock}
        \end{column}
        \begin{column}{0.55\textwidth}
            \begin{itemize}
                \item \code{export VEXCL_SHOW_KERNELS=1}\\
                    to see the generated code.
            \end{itemize}
        \end{column}
    \end{columns}
    \begin{exampleblock}{\ldots results in this kernel:}
        \begin{lstlisting}
kernel void vexcl_vector_kernel(
    ulong n,
    global double * prm_1,
    int prm_2,
    global double * prm_3,
    global double * prm_4
)
{
    for(size_t idx = get_global_id(0); idx < n; idx += get_global_size(0)) {
        prm_1[idx] = ( ( prm_2 * prm_3[idx] ) - sin( prm_4[idx] ) );
    }
}
        \end{lstlisting}
    \end{exampleblock}
    \begin{tikzpicture}[overlay,scale=0.6]
        \draw (16,8) node(sub)[draw,fill=white,ellipse,drop shadow]{$-$};

        \draw (sub) +(-2.00,-1) node(mul)[draw,fill=white,drop shadow,ellipse]{$*$};
        \draw (sub) +( 2.00,-1) node(sin)[draw,fill=white,drop shadow,ellipse]{sin};
        \draw (mul) +(-2.00,-1) node(two)[draw,fill=white,drop shadow,minimum size=0.5cm]{2};
        \draw (mul) +( 2.00,-1) node(y)  [draw,fill=white,drop shadow,minimum size=0.5cm]{y};
        \draw (sin) +( 1.75,-1) node(z)  [draw,fill=white,drop shadow,minimum size=0.5cm]{z};

        \draw (sub) -- (mul);
        \draw (sub) -- (sin);
        \draw (mul) -- (two);
        \draw (mul) -- (y);
        \draw (sin) -- (z);
    \end{tikzpicture}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}[fragile]{What vector expressions are supported?}
    \begin{itemize}
        \item All vectors in an expression have to be \emph{compatible}:
            \begin{itemize}
                \item Have same size
                \item Located on same devices
            \end{itemize}
            \vspace{\baselineskip}
        \item What may be used:
            \begin{columns}
                \begin{column}{0.4\textwidth}
                    \begin{itemize}
                        \item Vectors, scalars, constants
                        \item Arithmetic, logical operators
                        \item Built-in functions
                        \item User-defined functions
                        \item Random number generators
                        \item Slicing and permutations
                    \end{itemize}
                \end{column}
                \begin{column}{0.42\textwidth}
                    \begin{itemize}
                        \item Reduce to a scalar (sum, min, max)
                        \item Reduce across chosen dimensions
                        \item Stencil operations
                        \item Sparse matrix~-- vector products
                        \item Fast Fourier Transform
                        \item Sort, scan, reduce by key
                    \end{itemize}
                \end{column}
            \end{columns}
    \end{itemize}
\end{frame}

\note[itemize]{
\item So, what kind of expressions can you use in VexCL?
\item First, any vectors used in an expression have to be compatible.
\item If this requirement is satisfied, then expressions may combine
    vectors and scalars with almost any binary operators. OpenCL math functions
    and user-defined functions are also available.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Example: Monte Carlo $\pi$}
    \vspace{-1\baselineskip}
    \begin{columns}
        \begin{column}{0.55\textwidth}
            \begin{itemize}
                \item Compute approximate value of $\pi$:
            \end{itemize}
            \vspace{\baselineskip}
            \begin{equation*}
                \frac{\text{area of circle}}{\text{area of square}} =
                \frac{\pi r^2}{(2r)^2} = \frac{\pi}{4},
            \end{equation*}
            \begin{equation*}
                \pi = 4 \frac{\text{area of circle}}{\text{area of square}}
                \approx 4 \frac{\operatorname{count}(\text{\color{chameleon1}{points
                in circle}})}{\operatorname{count}(\text{\color{chameleon1}{all}
                \color{chameleon2}{points}})}
            \end{equation*}
        \end{column}
        \begin{column}{0.35\textwidth}
            \begin{figure}
                \includegraphics[width=\textwidth]{mcpi}
            \end{figure}
        \end{column}
    \end{columns}
    \begin{exampleblock}{}
        \begin{lstlisting}[texcl=true]
vex::Random<cl_double2> rnd; // Generates 2D points in $[0,1]\times[0,1]$
vex::Reductor<size_t, vex::SUM> sum(ctx);

double pi = 4.0 * sum( length( rnd(vex::element_index(0, n), seed) ) < 1 ) / n;
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

\note[itemize]{
\item Here is a bit more complex example of what you can do with VexCL.
\item Imagine we want to compute an approximate value of $\pi$ with Monte-Carlo
    method. We can use the following equalities to do this.
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Monte Carlo $\pi$: the generated kernel}
    \begin{columns}
        \begin{column}[t]{0.2\textwidth}
            \begin{exampleblock}{}
                \begin{adjustbox}{width=0.30\textwidth, height=\textheight, keepaspectratio}
                    \begin{minipage}{\textwidth}
                        \lstinputlisting[numbers=none,linerange={1-50}]{code/pi-kernel.cpp}
                    \end{minipage}
                \end{adjustbox}
            \end{exampleblock}
        \end{column}
        \begin{column}[t]{0.2\textwidth}
            \begin{exampleblock}{}
                \begin{adjustbox}{width=0.30\textwidth, height=\textheight, keepaspectratio}
                    \begin{minipage}{\textwidth}
                        \lstinputlisting[numbers=none,linerange={51-100}]{code/pi-kernel.cpp}
                    \end{minipage}
                \end{adjustbox}
            \end{exampleblock}
        \end{column}
        \begin{column}[t]{0.2\textwidth}
            \begin{exampleblock}{}
                \begin{adjustbox}{width=0.30\textwidth, height=\textheight, keepaspectratio}
                    \begin{minipage}{\textwidth}
                        \lstinputlisting[numbers=none,linerange={101-150}]{code/pi-kernel.cpp}
                    \end{minipage}
                \end{adjustbox}
            \end{exampleblock}
        \end{column}
        \begin{column}[t]{0.3\textwidth}
            \begin{exampleblock}{}
                \begin{adjustbox}{width=0.30\textwidth, height=\textheight, keepaspectratio}
                    \begin{minipage}{\textwidth}
                        \lstinputlisting[numbers=none,linerange={151-200}]{code/pi-kernel.cpp}
                    \end{minipage}
                \end{adjustbox}
            \end{exampleblock}
        \end{column}
    \end{columns}
\end{frame}

\note{}

%----------------------------------------------------------------------------
\section{Solving ODEs on GPUs}
\begin{frame}
    \sectionpage
\end{frame}

%----------------------------------------------------------------------------
\begin{frame}{Solving ODEs on GPUs}
    \vspace{-1\baselineskip}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{block}{Lorenz attractor system}
                \vspace{-1\baselineskip}
                \begin{align*}
                    \dot{x} &= -\sigma \left( x - y \right), \\
                    \dot{y} &= R x - y - xz, \\
                    \dot{z} &= -bz + xy.
                    \label{eq:lorenz}
                \end{align*}
            \end{block}
        \end{column}
        \begin{column}{0.4\textwidth}
            \begin{figure}
                \includegraphics[width=\textwidth]{lorenz}
            \end{figure}
        \end{column}
    \end{columns}
    \begin{itemize}
        \item Parameter study for the Lorenz attractor system:
            \begin{itemize}
                \item Solve large number of Lorenz systems, each
                    for a different value of $R$.
                \item Let's use Boost.odeint together with VexCL
                    for that.
            \end{itemize}
    \end{itemize}
    \footnoteline
    \begin{footnotesize}
        \begin{itemize}
            \item[{[1]}] K. Ahnert, D. Demidov, and M. Mulansky.  Solving
                ordinary differential equations on GPUs.\\
                In \emph{Numerical Computations with GPUs} (pp. 125-157).
                Springer, 2014.
                \href{http://dx.doi.org/10.1007/978-3-319-06548-9\_7}{doi:10.1007/978-3-319-06548-9\_7}
        \end{itemize}
    \end{footnotesize}
\end{frame}

%----------------------------------------------------------------------------
\begin{frame}{Using Boost.odeint}
    \begin{block}{Generic ODE:}
        \begin{gather*}
            \frac{\mbox{d} x}{\mbox{d} t } = \dot{x} = f(x , t),\\
             x(0) = x_0.
        \end{gather*}
    \end{block}

    \vspace{\baselineskip}

    \begin{exampleblock}{Using Boost.odeint:}
        \begin{itemize}
            \item Define state type (what is $x$?)
            \item Choose integration method
            \item Provide system function (define $f(x,t)$)
            \item Integrate over time
        \end{itemize}
    \end{exampleblock}
\end{frame}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Implementation}
    \setbeamercovered{transparent=40}
    \vspace{-1\baselineskip}
    \begin{columns}
        \begin{column}[c]{0.35\textwidth}
            \begin{exampleblock}{}
                \begin{adjustbox}{width=0.42\textwidth, height=\textheight, keepaspectratio}
                    \begin{minipage}{\textwidth}
                        \begin{uncoverenv}<1>
                            \lstinputlisting[numbers=none,linerange={1-7}]{code/lorenz.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1,2>
                            \lstinputlisting[numbers=none,linerange={8-14}]{code/lorenz.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1,3>
                            \lstinputlisting[numbers=none,linerange={15-26}]{code/lorenz.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1,4>
                            \lstinputlisting[numbers=none,linerange={27-45}]{code/lorenz.cpp}
                        \end{uncoverenv}
                    \end{minipage}
                \end{adjustbox}
            \end{exampleblock}
        \end{column}
        \begin{column}[c]{0.62\textwidth}
            \begin{onlyenv}<2>
                \begin{exampleblock}{1. State type and stepper type}
                    \begin{adjustbox}{width=0.75\textwidth, height=\textheight, keepaspectratio}
                        \begin{minipage}{\textwidth}
                            \lstinputlisting[firstnumber=9, linerange={9-14}]{code/lorenz.cpp}
                        \end{minipage}
                    \end{adjustbox}
                \end{exampleblock}
            \end{onlyenv}
            \begin{onlyenv}<3>
                \begin{exampleblock}{2. System function}
                    \begin{adjustbox}{width=0.73\textwidth, height=\textheight, keepaspectratio}
                        \begin{minipage}{\textwidth}
                            \lstinputlisting[firstnumber=10, linerange={16-26}]{code/lorenz.cpp}
                        \end{minipage}
                    \end{adjustbox}
                \end{exampleblock}
            \end{onlyenv}
            \begin{onlyenv}<4>
                \begin{exampleblock}{3. Integrate}
                    \begin{adjustbox}{width=0.75\textwidth, height=\textheight, keepaspectratio}
                        \begin{minipage}{\textwidth}
                            \lstinputlisting[firstnumber=27, linerange={28-45}]{code/lorenz.cpp}
                        \end{minipage}
                    \end{adjustbox}
                \end{exampleblock}
            \end{onlyenv}
        \end{column}
    \end{columns}
\end{frame}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Performance}{NVIDIA Tesla K40c, Intel Core i7 920
    \vspace{0.25\baselineskip}}
    \begin{figure}
        \includegraphics[width=0.8\textwidth]{cpu_vs_gpu_1}
    \end{figure}
    \vspace{-2\baselineskip}
    \begin{uncoverenv}<2>
        \begin{itemize}
            \item Problems:
                \begin{itemize}
                    \item Runge-Kutta method uses 4 temporary vectors.
                    \item Single Runge-Kutta step is translated into several
                        kernels.
                \end{itemize}
        \end{itemize}
    \end{uncoverenv}
\end{frame}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{What if we did this manually?}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item Create monolithic kernel for a single step of the
                    Runge-Kutta method.
                \item Launch the kernel in a loop.
                    \vspace{\baselineskip}
                \item This would be 10x faster!
                    \begin{itemize}
                        \item \alert{At the expense of odeint's generality.}
                    \end{itemize}
            \end{itemize}
        \end{column} \quad \quad
        \hspace{-3em}
        \begin{column}{0.45\textwidth}
            \begin{exampleblock}{Monolithic OpenCL kernel for RK4}
                \begin{adjustbox}{width=\textwidth,height=\textheight,keepaspectratio}
                    \begin{lstlisting}
double3 lorenz_system(double r, double sigma, double b, double3 s) {
    return (double3)( sigma * (s.y - s.x),
                       r * s.x - s.y - s.x * s.z,
                       s.x * s.y - b * s.z);
}
kernel void lorenz_ensemble(
    ulong n, double dt, double sigma, double b,
    const global double *R,
    global double *X,
    global double *Y,
    global double *Z
    )
{
    for(size_t i = get_global_id(0); i < n; i += get_global_size(0)) {
        double  r = R[i];
        double3 s = (double3)(X[i], Y[i], Z[i]);
        double3 k1, k2, k3, k4;

        k1 = dt * lorenz_system(r, sigma, b, s);
        k2 = dt * lorenz_system(r, sigma, b, s + 0.5 * k1);
        k3 = dt * lorenz_system(r, sigma, b, s + 0.5 * k2);
        k4 = dt * lorenz_system(r, sigma, b, s + k3);

        s += (k1 + 2 * k2 + 2 * k3 + k4) / 6;

        X[i] = s.x; Y[i] = s.y; Z[i] = s.z;
    }
}
                    \end{lstlisting}
                \end{adjustbox}
            \end{exampleblock}
        \end{column}
    \end{columns}
\end{frame}

\note[itemize]{
\item So, what if we did this manually?
\item We would create a single kernel that would do complete Runge-Kutta
    integration step. By the way, here is the kernel that does just that. It's
    very nice-looking kernel in fact.
\item If we run this kernel in a loop, it would give us our solution. And it
    would be ten times faster than our previous variant. So a hundred times
    faster than a CPU! That's an acceleration!
\item But, odeint has 20 different steppers. We don't want to reimplement all
    of those. Let Karsten here do the job, right?
}

\subsection{Kernel generator}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{VexCL symbolic types}
    \begin{itemize}
        \item VexCL provides \code{vex::symbolic<T>} class template.
        \item Its instance dumps to an \code{std::ostream} any
            arithmetic operations applied to it:
    \end{itemize}
    \begin{columns}
        \begin{column}{0.35\textwidth}
            \begin{exampleblock}{}
                \begin{lstlisting}
vex::symbolic<double> x = 6;
vex::symbolic<double> y = 7;
x = sin(x * y);
                \end{lstlisting}
            \end{exampleblock}
            \begin{small}
                \begin{verbatim}
double var1 = 6;
double var2 = 7;
var1 = sin( ( var1 * var2 ) );
                \end{verbatim}
            \end{small}
        \end{column}
        \begin{column}<2>{0.5\textwidth}
            \begin{exampleblock}{}
                \begin{lstlisting}
template <class T>
T squared_radius(const T &x, const T &y) {
    return x * x + y * y;
}
template <class T>
T radius(const T &x, const T &y) {
    return sqrt(squared_radius(x, y));
}
vex::symbolic<double> a = 3, b = 4;
vex::symbolic<double> c = radius(a, b);
                \end{lstlisting}
            \end{exampleblock}
            \begin{small}
                \begin{verbatim}
double var1 = 3;
double var2 = 4;
double var3 = ( ( var1 * var1 ) + ( var2 * var2 ) );
double var4 = sqrt( var3 );
                \end{verbatim}
            \end{small}
        \end{column}
    \end{columns}
\end{frame}

\note[itemize]{
\item VexCL allows to achieve same effect without manual coding.
\item The idea is very simple:
    \begin{itemize}
        \item An algorithm (any algorithm) is just a sequence of arithmetic
            expressions.
        \item VexCL symbolic types allow to record such expressions.
    \end{itemize}
}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Generate monolithic kernel using Boost.odeint algorithm}
    \begin{itemize}
        \item Overview (see \alert{[1]} for more details):
        \begin{itemize}
            \item Use \code{vex::symbolic<T>} or
                \code{boost::array<vex::symbolic<T>, N>} as state type.
            \item Provide generic system functor.
            \item Record single step of a Boost.odeint stepper.
            \item Generate and use monolithic kernel. 
        \end{itemize}
        \vspace{\baselineskip}
    \item<2> Restrictions:
        \begin{itemize}
            \item Algorithms have to be embarrassingly parallel.
            \item Only linear flow is allowed (no conditionals or
                data-dependent loops).
        \end{itemize}
    \end{itemize}
    \footnoteline
    \begin{footnotesize}
        \begin{itemize}
            \item[{[1]}] K. Ahnert, D. Demidov, and M. Mulansky.  Solving
                ordinary differential equations on GPUs.\\
                In \emph{Numerical Computations with GPUs} (pp. 125-157).
                Springer, 2014.
                \href{http://dx.doi.org/10.1007/978-3-319-06548-9\_7}{doi:10.1007/978-3-319-06548-9\_7}
        \end{itemize}
    \end{footnotesize}
\end{frame}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Performance of the generated kernel}
    \begin{figure}
        \includegraphics[width=\textwidth]{cpu_vs_gpu_2}
    \end{figure}
\end{frame}

\note[itemize]{
\item But, as you can see from this slide, the technique allows to achieve same
    acceleration we got from manually coded kernel (Both for CPU and GPU).
}

%----------------------------------------------------------------------------
\begin{frame}{Comparing backends performance}
    \begin{figure}
        \includegraphics[width=\textwidth]{backends}
    \end{figure}
\end{frame}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Summary}
    \begin{columns}
        \begin{column}{0.65\textwidth}
            \begin{itemize}
                \item VexCL allows to write compact and readable code.
                    \begin{itemize}
                        \item It's great for fast prototyping of scientific
                            GPGPU applications.
                            \vspace{0.5\baselineskip}
                        \item The performance is often on-par with manually
                            written kernels.
                            \vspace{0.5\baselineskip}
                        \item \emph{Using custom kernels or third-party
                            functionality is still possible.}
                    \end{itemize}
                    \vspace{\baselineskip}
                    \pause
                \item It stresses the compiler greatly.
                    \begin{itemize}
                        \item Compilation takes a lot of RAM and a lot of
                            time.
                        \item But, it's easy to combine \CXX with Python:
                            \www{http://pybind11.readthedocs.io}
                    \end{itemize}
            \end{itemize}
        \end{column}
        \begin{column}{0.3\textwidth}
            \begin{uncoverenv}<2>
                \begin{figure}
                    \includegraphics[width=\textwidth]{compiling.png}
                    \caption{\tiny \www{https://xkcd.com/303/}}
                \end{figure}
            \end{uncoverenv}
        \end{column}
    \end{columns}
\end{frame}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Creating python bindings for \CXX with pybind11}
    \setbeamercovered{transparent=40}
    \vspace{-0.5\baselineskip}
    \begin{columns}
        \begin{column}[c]{0.3\textwidth}
            \begin{exampleblock}{}
                \begin{adjustbox}{width=0.28\textwidth, height=\textheight, keepaspectratio}
                    \begin{minipage}{\textwidth}
                        \begin{uncoverenv}<1>
                            \lstinputlisting[numbers=none, linerange={1-10}]{code/pylorenz.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1,2>
                            \lstinputlisting[numbers=none, linerange={11-17}]{code/pylorenz.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1,3>
                            \lstinputlisting[numbers=none, linerange={18-22}]{code/pylorenz.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1,4>
                            \lstinputlisting[numbers=none, linerange={23-52}]{code/pylorenz.cpp}
                        \end{uncoverenv}
                        \begin{uncoverenv}<1,5>
                            \lstinputlisting[numbers=none, linerange={53-67}]{code/pylorenz.cpp}
                        \end{uncoverenv}
                    \end{minipage}
                \end{adjustbox}
            \end{exampleblock}
        \end{column}
        \begin{column}[c]{0.6\textwidth}
            \begin{onlyenv}<2>
                \begin{exampleblock}{1. State type and stepper type}
                    \begin{adjustbox}{width=0.78\textwidth, height=\textheight, keepaspectratio}
                        \begin{minipage}{\textwidth}
                            \lstinputlisting[firstnumber=12, linerange={12-17}]{code/pylorenz.cpp}
                        \end{minipage}
                    \end{adjustbox}
                \end{exampleblock}
            \end{onlyenv}
            \begin{onlyenv}<3>
                \begin{exampleblock}{2. Context initialization}
                    \begin{adjustbox}{width=0.75\textwidth, height=\textheight, keepaspectratio}
                        \begin{minipage}{\textwidth}
                            \lstinputlisting[firstnumber=19, linerange={19-22}]{code/pylorenz.cpp}
                        \end{minipage}
                    \end{adjustbox}
                \end{exampleblock}
            \end{onlyenv}
            \begin{onlyenv}<4>
                \begin{exampleblock}{3. System function}
                    \begin{adjustbox}{width=0.6\textwidth, height=\textheight, keepaspectratio}
                        \begin{minipage}{\textwidth}
                            \lstinputlisting[firstnumber=24, linerange={24-52}]{code/pylorenz.cpp}
                        \end{minipage}
                    \end{adjustbox}
                \end{exampleblock}
            \end{onlyenv}
            \begin{onlyenv}<5>
                \begin{exampleblock}{4. Python bindings}
                    \begin{adjustbox}{width=0.78\textwidth, height=\textheight, keepaspectratio}
                        \begin{minipage}{\textwidth}
                            \lstinputlisting[firstnumber=54, linerange={54-67}]{code/pylorenz.cpp}
                        \end{minipage}
                    \end{adjustbox}
                \end{exampleblock}
            \end{onlyenv}
        \end{column}
    \end{columns}
\end{frame}

%----------------------------------------------------------------------------
\begin{frame}[fragile]{Calling \CXX code from Python}
    \begin{exampleblock}{}
        \begin{lstlisting}[language=python]
import pylorenz as lorenz

n = 32   # Number of parameters
m = 500  # Length of trajectory

# Parameters
R = linspace(0.1, 10, n)
sigma = 10.0
b = 8/3

# Initial solution
S = empty((m, 3, n))
S[0,:,:] = 10.0

# Integration
ode = lorenz.Stepper(sigma, b, R)
for i in range(1,m):
    S[i] = ode.advance(S[i-1], 1, 0.01)
        \end{lstlisting}
    \end{exampleblock}
\end{frame}

%----------------------------------------------------------------------------
\begin{frame}{Projects using VexCL}
    \begin{description}[\quad]
        \item[AMGCL] --- an efficient, flexible, and extensible Algebraic
            Multigrid implementation:
            \begin{itemize}
                \item \www{https://github.com/ddemidov/amgcl}
            \end{itemize}
        \item[Boost.odeint] --- numerical solution of Ordinary Differential
            Equations:
            \begin{itemize}
                \item \www{https://github.com/boostorg/odeint}
            \end{itemize}
        \item[Antioch] --- A New Templated Implementation Of Chemistry for
            Hydrodynamics\\(The University of Texas at Austin):
            \begin{itemize}
                \item \www{https://github.com/libantioch/antioch}
            \end{itemize}
        \item[FDBB] --- Fluid Dynamics Building Blocks (TU Delft):
            \begin{itemize}
                \item \www{https://gitlab.com/mmoelle1/FDBB}
            \end{itemize}
    \end{description}
\end{frame}

\note{ }

%----------------------------------------------------------------------------
\begin{frame}{}
    \begin{description}[Documentation:]
        \item[Source code:] \www{https://github.com/ddemidov/vexcl}
            \vspace{\baselineskip}
        \item[Documentation:] \www{http://vexcl.readthedocs.io}
            \vspace{\baselineskip}
        \item[Slides:] \www{https://speakerdeck.com/ddemidov}
            \vspace{\baselineskip}
        \item[Example codes:] \www{https://github.com/ddemidov/vexcl-talks}
    \end{description}
\end{frame}

\end{document}
